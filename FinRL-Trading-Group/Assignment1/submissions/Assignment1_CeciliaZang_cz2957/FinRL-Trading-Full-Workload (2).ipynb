{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GR5398 26 Spring: FinRL-Trading Quantitative Trading Strategy Track\n",
    "## Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment 1, we want you to :\n",
    "+ Run this ipynb file, which is a simplified FinRL-Trading whole process, and have a basic knowledge of what we will do in this semester\n",
    "+ Design a portfolio using the selected stocks, and learn some fundamental information of quantitative trading (especially stock selection part)\n",
    "+ Implement a full backtest process to verify your strategy's performance using real historical data\n",
    "+ Summarize your result in a very brief research report, and write a `Medium Blog`. Submit your code files onto GitHub repo in a new folder called Assignment1_Name_UNI.\n",
    "    + An example of medium blog: [Applying new LLMs on FinGPT: Fine-tune DeepSeek and Llama3](https://medium.com/p/6ac9198d88b2)\n",
    "\n",
    "Assignment 1 Report Submission Due Day: **Feb 20, 2026**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For full `FinRL-Trading` project, please refer to [AI4Finance/FinRL-Trading](https://github.com/AI4Finance-Foundation/FinRL-Trading/tree/master_backup)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Prerequisities\n",
    "\n",
    "First, you should decide your stock pool for later selection. Here we recommend you to choose your portfolio from S&P 500 index or NASDAQ 100 index. We have already provided you with their components.\n",
    "\n",
    "Then, you should download stock's daily OHLCV data on [WRDS-Security Daily](https://wrds-www.wharton.upenn.edu/pages/get-data/compustat-capital-iq-standard-poors/compustat/north-america-daily/security-daily/) and quarterly fundamental data on [WRDS-Fundamental Quarterly](https://wrds-www.wharton.upenn.edu/pages/get-data/compustat-capital-iq-standard-poors/compustat/north-america-daily/security-daily/) for later usage.\n",
    "\n",
    "+ Note: Here we highly recommend you to register for a WRDS account which our university has provided for all master students for free. Please follow the instruction below to get a WRDS account:\n",
    "    + https://guides.library.columbia.edu/wrds\n",
    "\n",
    "If you don't have enough storage or want this data file to be loaded faster while doing calculation, you can select these columns below only while querying the WRDS database.\n",
    "+ prccd (Price - Close - Daily)\n",
    "+ prcod (Price - Open - Daily)\n",
    "+ ajexdi (Adjustment Factor (Issue)-Cumulative by Ex-Date)\n",
    "+ tic (Ticker)\n",
    "\n",
    "Since we want you to implement backtest from **Jan 1, 2018** to **Dec 31, 2025**, we suggest you to download all the data during this period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing\n",
    "\n",
    "In this part, we will preprocess all the fundamental data for our Machine Learning algorithms. Please refer to [AI4Finance/FinRL-Trading/data_processor/Step2_preprocess_fundmental_data.py](https://github.com/AI4Finance-Foundation/FinRL-Trading/blob/master_backup/data_processor/Step2_preprocess_fundmental_data.py) for detailed usage.\n",
    "\n",
    "After running this part, you should get a folder with final fundamental ratios split into different industry sections in `xlsx` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from datetime import datetime as dt\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fundamental_file, price_file):\n",
    "    \"\"\"\n",
    "    Load fundamental and price data from CSV files.\n",
    "    \n",
    "    Args:\n",
    "        fundamental_file (str): Path to fundamental data CSV file\n",
    "        price_file (str): Path to price data CSV file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (fundamental_df, price_df)\n",
    "    \"\"\"\n",
    "    print(\"Loading data files...\")\n",
    "    \n",
    "    if not os.path.isfile(fundamental_file):\n",
    "        raise FileNotFoundError(f\"Fundamental file {fundamental_file} not found.\")\n",
    "    \n",
    "    if not os.path.isfile(price_file):\n",
    "        raise FileNotFoundError(f\"Price file {price_file} not found.\")\n",
    "    \n",
    "    # Load fundamental data\n",
    "    fund_df = pd.read_csv(fundamental_file)\n",
    "    print(fund_df.head())\n",
    "    \n",
    "    # For price data, only load necessary columns to save memory\n",
    "    print(\"Loading price data (only necessary columns)...\")\n",
    "    price_columns = ['gvkey', 'tic', 'datadate', 'prccd', 'ajexdi']\n",
    "    df_daily_price = pd.read_csv(price_file, usecols=price_columns)\n",
    "    print(df_daily_price.head())\n",
    "    \n",
    "    print(f\"Fundamental data shape: {fund_df.shape}\")\n",
    "    print(f\"Price data shape: {df_daily_price.shape}\")\n",
    "    print(f\"Unique tickers in fundamental data: {len(fund_df.tic.unique())}\")\n",
    "    print(f\"Unique tickers in price data: {len(df_daily_price.tic.unique())}\")\n",
    "    \n",
    "    return fund_df, df_daily_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_trade_dates(fund_df):\n",
    "    \"\"\"\n",
    "    Adjust trade dates to use trading dates instead of quarterly report dates.\n",
    "    \n",
    "    Args:\n",
    "        fund_df (pandas.DataFrame): Fundamental data DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with adjusted trade dates\n",
    "    \"\"\"\n",
    "    print(\"Adjusting trade dates...\")\n",
    "    \n",
    "    # Convert datadate to datetime first, then to integer format\n",
    "    datadate_dt = pd.to_datetime(fund_df['datadate'])\n",
    "\n",
    "    fund_df['tradedate'] = ((datadate_dt.dt.to_period('Q')).dt.end_time.dt.normalize())\n",
    "    fund_df['reportdate'] = fund_df[\"rdq\"]\n",
    "    \n",
    "    return fund_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adjusted_close(fund_df):\n",
    "    \"\"\"\n",
    "    Calculate adjusted close price.\n",
    "    \n",
    "    Args:\n",
    "        fund_df (pandas.DataFrame): Fundamental data DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with adjusted close price\n",
    "    \"\"\"\n",
    "    print(\"Calculating adjusted close price...\")\n",
    "    fund_df['adj_close_q'] = fund_df.prccq / fund_df.adjex\n",
    "    return fund_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_tickers_and_gvkey(fund_df, df_daily_price):\n",
    "    \"\"\"\n",
    "    Match tickers and gvkey for fundamental and price data.\n",
    "    \n",
    "    Args:\n",
    "        fund_df (pandas.DataFrame): Fundamental data DataFrame\n",
    "        df_daily_price (pandas.DataFrame): Price data DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Filtered fundamental data DataFrame\n",
    "    \"\"\"\n",
    "    print(\"Matching tickers and gvkey...\")\n",
    "    \n",
    "    # Create mapping from ticker to gvkey\n",
    "    tic_to_gvkey = {}\n",
    "    df_daily_groups = list(df_daily_price.groupby('tic'))\n",
    "    \n",
    "    for tic, df_ in df_daily_groups:\n",
    "        tic_to_gvkey[tic] = df_.gvkey.iloc[0]\n",
    "    \n",
    "    print(f\"Original fundamental data shape: {fund_df.shape}\")\n",
    "    \n",
    "    # Filter fundamental data to only include tickers present in price data\n",
    "    fund_df = fund_df[np.isin(fund_df.tic, list(tic_to_gvkey.keys()))]\n",
    "    \n",
    "    print(f\"Filtered fundamental data shape: {fund_df.shape}\")\n",
    "    print(f\"Unique gvkeys: {len(fund_df.gvkey.unique())}\")\n",
    "    \n",
    "    # Add gvkey mapping\n",
    "    fund_df['gvkey'] = [tic_to_gvkey[x] for x in fund_df['tic']]\n",
    "    \n",
    "    return fund_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_next_quarter_returns(fund_df):\n",
    "    \"\"\"\n",
    "    Calculate next quarter's return for each stock.\n",
    "    \n",
    "    Args:\n",
    "        fund_df (pandas.DataFrame): Fundamental data DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with next quarter returns\n",
    "    \"\"\"\n",
    "    print(\"Calculating next quarter returns...\")\n",
    "    \n",
    "    fund_df['date'] = fund_df[\"tradedate\"]\n",
    "    # fund_df['date'] = pd.to_datetime(fund_df['date'], format=\"%Y-%m%d\")\n",
    "    fund_df['date'] = pd.to_datetime(fund_df['date'])\n",
    "    fund_df.drop_duplicates([\"date\", \"gvkey\"], keep='last', inplace=True)\n",
    "    \n",
    "    # Calculate next quarter return for each stock\n",
    "    l_df = list(fund_df.groupby('gvkey'))\n",
    "    for tic, df in l_df:\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        df.sort_values('date')\n",
    "        # Calculate next quarter's return\n",
    "        df['y_return'] = np.log(df['adj_close_q'].shift(-1) / df['adj_close_q'])\n",
    "    \n",
    "    fund_df = pd.concat([x[1] for x in l_df])\n",
    "    \n",
    "    print(f\"Data shape after calculating returns: {fund_df.shape}\")\n",
    "    return fund_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_basic_ratios(fund_df):\n",
    "    \"\"\"\n",
    "    Calculate basic financial ratios (PE, PS, PB).\n",
    "    \n",
    "    Args:\n",
    "        fund_df (pandas.DataFrame): Fundamental data DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with basic ratios\n",
    "    \"\"\"\n",
    "    print(\"Calculating basic financial ratios...\")\n",
    "    \n",
    "    fund_df['pe'] = fund_df.prccq / fund_df.epspxq\n",
    "    fund_df['ps'] = fund_df.prccq / (fund_df.revtq / fund_df.cshoq)\n",
    "    fund_df['pb'] = fund_df.prccq / ((fund_df.atq - fund_df.ltq) / fund_df.cshoq)\n",
    "    \n",
    "    return fund_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(fund_df):\n",
    "    \"\"\"\n",
    "    Select relevant columns for analysis.\n",
    "    \n",
    "    Args:\n",
    "        fund_df (pandas.DataFrame): Fundamental data DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with selected columns\n",
    "    \"\"\"\n",
    "    print(\"Selecting relevant columns...\")\n",
    "    \n",
    "    items = [\n",
    "        'date', 'gvkey', 'tic', 'gsector',\n",
    "        'oiadpq', 'revtq', 'niq', 'atq', 'teqq', 'epspiy', 'ceqq', 'cshoq', 'dvpspq',\n",
    "        'actq', 'lctq', 'cheq', 'rectq', 'cogsq', 'invtq', 'apq', 'dlttq', 'dlcq', 'ltq',\n",
    "        'pe', 'ps', 'pb', 'adj_close_q', 'y_return', 'reportdate'\n",
    "    ]\n",
    "    \n",
    "    fund_data = fund_df[items]\n",
    "    \n",
    "    # Rename columns for readability\n",
    "    fund_data = fund_data.rename(columns={\n",
    "        'oiadpq': 'op_inc_q',\n",
    "        'revtq': 'rev_q',\n",
    "        'niq': 'net_inc_q',\n",
    "        'atq': 'tot_assets',\n",
    "        'teqq': 'sh_equity',\n",
    "        'epspiy': 'eps_incl_ex',\n",
    "        'ceqq': 'com_eq',\n",
    "        'cshoq': 'sh_outstanding',\n",
    "        'dvpspq': 'div_per_sh',\n",
    "        'actq': 'cur_assets',\n",
    "        'lctq': 'cur_liabilities',\n",
    "        'cheq': 'cash_eq',\n",
    "        'rectq': 'receivables',\n",
    "        'cogsq': 'cogs_q',\n",
    "        'invtq': 'inventories',\n",
    "        'apq': 'payables',\n",
    "        'dlttq': 'long_debt',\n",
    "        'dlcq': 'short_debt',\n",
    "        'ltq': 'tot_liabilities'\n",
    "    })\n",
    "    \n",
    "    return fund_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_financial_ratios(fund_data):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive financial ratios.\n",
    "    \n",
    "    Args:\n",
    "        fund_data (pandas.DataFrame): Fundamental data DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with all financial ratios\n",
    "    \"\"\"\n",
    "    print(\"Calculating comprehensive financial ratios...\")\n",
    "    \n",
    "    # Set data type to series\n",
    "    date = fund_data['date'].to_frame('date').reset_index(drop=True)\n",
    "    reportdate = fund_data['reportdate'].to_frame('reportdate').reset_index(drop=True)\n",
    "    tic = fund_data['tic'].to_frame('tic').reset_index(drop=True)\n",
    "    gvkey = fund_data['gvkey'].to_frame('gvkey').reset_index(drop=True)\n",
    "    adj_close_q = fund_data['adj_close_q'].to_frame('adj_close_q').reset_index(drop=True)\n",
    "    y_return = fund_data['y_return'].to_frame('y_return').reset_index(drop=True)\n",
    "    gsector = fund_data['gsector'].to_frame('gsector').reset_index(drop=True)\n",
    "    pe = fund_data['pe'].to_frame('pe').reset_index(drop=True)\n",
    "    ps = fund_data['ps'].to_frame('ps').reset_index(drop=True)\n",
    "    pb = fund_data['pb'].to_frame('pb').reset_index(drop=True)\n",
    "    \n",
    "    # Profitability ratios\n",
    "    print(\"  Calculating profitability ratios...\")\n",
    "    \n",
    "    # Operating Margin\n",
    "    OPM = pd.Series(np.empty(fund_data.shape[0], dtype=object), name='OPM')\n",
    "    for i in range(0, fund_data.shape[0]):\n",
    "        if i-3 < 0:\n",
    "            OPM[i] = np.nan\n",
    "        elif fund_data.iloc[i, 1] != fund_data.iloc[i-3, 1]:\n",
    "            OPM.iloc[i] = np.nan\n",
    "        else:\n",
    "            OPM.iloc[i] = np.sum(fund_data['op_inc_q'].iloc[i-3:i]) / np.sum(fund_data['rev_q'].iloc[i-3:i])\n",
    "    OPM = pd.Series(OPM).to_frame().reset_index(drop=True)\n",
    "    \n",
    "    # Net Profit Margin\n",
    "    NPM = pd.Series(np.empty(fund_data.shape[0], dtype=object), name='NPM')\n",
    "    for i in range(0, fund_data.shape[0]):\n",
    "        if i-3 < 0:\n",
    "            NPM[i] = np.nan\n",
    "        elif fund_data.iloc[i, 1] != fund_data.iloc[i-3, 1]:\n",
    "            NPM.iloc[i] = np.nan\n",
    "        else:\n",
    "            NPM.iloc[i] = np.sum(fund_data['net_inc_q'].iloc[i-3:i]) / np.sum(fund_data['rev_q'].iloc[i-3:i])\n",
    "    NPM = pd.Series(NPM).to_frame().reset_index(drop=True)\n",
    "    \n",
    "    # Return On Assets\n",
    "    ROA = pd.Series(np.empty(fund_data.shape[0], dtype=object), name='ROA')\n",
    "    for i in range(0, fund_data.shape[0]):\n",
    "        if i-3 < 0:\n",
    "            ROA[i] = np.nan\n",
    "        elif fund_data.iloc[i, 1] != fund_data.iloc[i-3, 1]:\n",
    "            ROA.iloc[i] = np.nan\n",
    "        else:\n",
    "            ROA.iloc[i] = np.sum(fund_data['net_inc_q'].iloc[i-3:i]) / fund_data['tot_assets'].iloc[i]\n",
    "    ROA = pd.Series(ROA).to_frame().reset_index(drop=True)\n",
    "    \n",
    "    # Return on Equity\n",
    "    ROE = pd.Series(np.empty(fund_data.shape[0], dtype=object), name='ROE')\n",
    "    for i in range(0, fund_data.shape[0]):\n",
    "        if i-3 < 0:\n",
    "            ROE[i] = np.nan\n",
    "        elif fund_data.iloc[i, 1] != fund_data.iloc[i-3, 1]:\n",
    "            ROE.iloc[i] = np.nan\n",
    "        else:\n",
    "            ROE.iloc[i] = np.sum(fund_data['net_inc_q'].iloc[i-3:i]) / fund_data['sh_equity'].iloc[i]\n",
    "    ROE = pd.Series(ROE).to_frame().reset_index(drop=True)\n",
    "    \n",
    "    # Per share items\n",
    "    EPS = fund_data['eps_incl_ex'].to_frame('EPS').reset_index(drop=True)\n",
    "    BPS = (fund_data['com_eq'] / fund_data['sh_outstanding']).to_frame('BPS').reset_index(drop=True)\n",
    "    DPS = fund_data['div_per_sh'].to_frame('DPS').reset_index(drop=True)\n",
    "    \n",
    "    # Liquidity ratios\n",
    "    print(\"  Calculating liquidity ratios...\")\n",
    "    cur_ratio = (fund_data['cur_assets'] / fund_data['cur_liabilities']).to_frame('cur_ratio').reset_index(drop=True)\n",
    "    quick_ratio = ((fund_data['cash_eq'] + fund_data['receivables']) / fund_data['cur_liabilities']).to_frame('quick_ratio').reset_index(drop=True)\n",
    "    cash_ratio = (fund_data['cash_eq'] / fund_data['cur_liabilities']).to_frame('cash_ratio').reset_index(drop=True)\n",
    "    \n",
    "    # Efficiency ratios\n",
    "    print(\"  Calculating efficiency ratios...\")\n",
    "    \n",
    "    # Inventory turnover ratio\n",
    "    inv_turnover = pd.Series(np.empty(fund_data.shape[0], dtype=object), name='inv_turnover')\n",
    "    for i in range(0, fund_data.shape[0]):\n",
    "        if i-3 < 0:\n",
    "            inv_turnover[i] = np.nan\n",
    "        elif fund_data.iloc[i, 1] != fund_data.iloc[i-3, 1]:\n",
    "            inv_turnover.iloc[i] = np.nan\n",
    "        else:\n",
    "            inv_turnover.iloc[i] = np.sum(fund_data['cogs_q'].iloc[i-3:i]) / fund_data['inventories'].iloc[i]\n",
    "    inv_turnover = pd.Series(inv_turnover).to_frame().reset_index(drop=True)\n",
    "    \n",
    "    # Receivables turnover ratio\n",
    "    acc_rec_turnover = pd.Series(np.empty(fund_data.shape[0], dtype=object), name='acc_rec_turnover')\n",
    "    for i in range(0, fund_data.shape[0]):\n",
    "        if i-3 < 0:\n",
    "            acc_rec_turnover[i] = np.nan\n",
    "        elif fund_data.iloc[i, 1] != fund_data.iloc[i-3, 1]:\n",
    "            acc_rec_turnover.iloc[i] = np.nan\n",
    "        else:\n",
    "            acc_rec_turnover.iloc[i] = np.sum(fund_data['rev_q'].iloc[i-3:i]) / fund_data['receivables'].iloc[i]\n",
    "    acc_rec_turnover = pd.Series(acc_rec_turnover).to_frame().reset_index(drop=True)\n",
    "    \n",
    "    # Payable turnover ratio\n",
    "    acc_pay_turnover = pd.Series(np.empty(fund_data.shape[0], dtype=object), name='acc_pay_turnover')\n",
    "    for i in range(0, fund_data.shape[0]):\n",
    "        if i-3 < 0:\n",
    "            acc_pay_turnover[i] = np.nan\n",
    "        elif fund_data.iloc[i, 1] != fund_data.iloc[i-3, 1]:\n",
    "            acc_pay_turnover.iloc[i] = np.nan\n",
    "        else:\n",
    "            acc_pay_turnover.iloc[i] = np.sum(fund_data['cogs_q'].iloc[i-3:i]) / fund_data['payables'].iloc[i]\n",
    "    acc_pay_turnover = pd.Series(acc_pay_turnover).to_frame().reset_index(drop=True)\n",
    "    \n",
    "    # Leverage ratios\n",
    "    print(\"  Calculating leverage ratios...\")\n",
    "    debt_ratio = (fund_data['tot_liabilities'] / fund_data['tot_assets']).to_frame('debt_ratio').reset_index(drop=True)\n",
    "    debt_to_equity = (fund_data['tot_liabilities'] / fund_data['sh_equity']).to_frame('debt_to_equity').reset_index(drop=True)\n",
    "    \n",
    "    # Create final ratios dataframe\n",
    "    ratios = pd.concat([\n",
    "        date, gvkey, tic, gsector, adj_close_q, y_return,\n",
    "        OPM, NPM, ROA, ROE, EPS, BPS, DPS,\n",
    "        cur_ratio, quick_ratio, cash_ratio, inv_turnover, acc_rec_turnover, acc_pay_turnover,\n",
    "        debt_ratio, debt_to_equity, pe, ps, pb, reportdate\n",
    "    ], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    return ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(ratios):\n",
    "    \"\"\"\n",
    "    Handle missing values and infinite values in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        ratios (pandas.DataFrame): DataFrame with financial ratios\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    print(\"Handling missing values...\")\n",
    "    \n",
    "    # Replace NAs and infinite values with zero initially\n",
    "    final_ratios = ratios.copy()\n",
    "    final_ratios = final_ratios.fillna(0)\n",
    "    final_ratios = final_ratios.replace(np.inf, 0)\n",
    "    \n",
    "    # Define financial features columns\n",
    "    features_column_financial = [\n",
    "        'OPM', 'NPM', 'ROA', 'ROE', 'EPS', 'BPS', 'DPS', 'cur_ratio',\n",
    "        'quick_ratio', 'cash_ratio', 'inv_turnover', 'acc_rec_turnover',\n",
    "        'acc_pay_turnover', 'debt_ratio', 'debt_to_equity', 'pe', 'ps', 'pb'\n",
    "    ]\n",
    "    \n",
    "    # Remove rows with zero adjusted close price\n",
    "    final_ratios = final_ratios.drop(list(final_ratios[final_ratios.adj_close_q == 0].index)).reset_index(drop=True)\n",
    "    \n",
    "    # Convert to numeric and handle invalid values\n",
    "    final_ratios['y_return'] = pd.to_numeric(final_ratios['y_return'], errors='coerce')\n",
    "    for col in features_column_financial:\n",
    "        if col in final_ratios.columns:\n",
    "            final_ratios[col] = pd.to_numeric(final_ratios[col], errors='coerce')\n",
    "    \n",
    "    final_ratios['y_return'].replace([np.nan, np.inf, -np.inf], np.nan, inplace=True)\n",
    "    final_ratios[features_column_financial].replace([np.nan, np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # Remove columns with too many invalid values\n",
    "    dropped_col = []\n",
    "    for col in features_column_financial:\n",
    "        if col in final_ratios.columns and np.any(~np.isfinite(final_ratios[col])):\n",
    "            final_ratios.drop(columns=[col], axis=1, inplace=True)\n",
    "            dropped_col.append(col)\n",
    "    \n",
    "    # Remove rows with any missing values\n",
    "    final_ratios.dropna(axis=0, inplace=True)\n",
    "    final_ratios = final_ratios[final_ratios[\"reportdate\"].ne(0)]  \n",
    "    final_ratios = final_ratios.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Dropped columns: {dropped_col}\")\n",
    "    print(f\"Final data shape: {final_ratios.shape}\")\n",
    "    \n",
    "    return final_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(final_ratios, output_dir=\"outputs\", include_sector0=False):\n",
    "    \"\"\"\n",
    "    Save the processed data to files.\n",
    "    \n",
    "    Args:\n",
    "        final_ratios (pandas.DataFrame): Final processed data\n",
    "        output_dir (str): Output directory\n",
    "        include_sector0 (bool): Whether to include sector 0 in sector-specific files (default: False)\n",
    "    \"\"\"\n",
    "    print(\"Saving results...\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Format date column\n",
    "    # final_ratios.date = final_ratios.date.apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    \n",
    "    # Save main results\n",
    "    main_output_file = os.path.join(output_dir, 'final_ratios.csv')\n",
    "    final_ratios.to_csv(main_output_file, index=False)\n",
    "    print(f\"Main results saved to: {main_output_file}\")\n",
    "    \n",
    "    # Save by sector\n",
    "    print(\"Saving sector-specific files...\")\n",
    "    sector_count = 0\n",
    "    for sec, df_ in list(final_ratios.groupby('gsector')):\n",
    "        # Skip sector 0 unless explicitly included\n",
    "        if sec == 0 and not include_sector0:\n",
    "            print(f\"  Skipping Sector 0: {len(df_)} records (stocks with missing sector information)\")\n",
    "            continue\n",
    "        \n",
    "        sector_file = os.path.join(output_dir, f\"sector{int(sec)}.xlsx\")\n",
    "        df_.to_excel(sector_file, index=False)\n",
    "        print(f\"  Sector {int(sec)}: {sector_file} ({len(df_)} records)\")\n",
    "        sector_count += 1\n",
    "    \n",
    "    print(f\"  Total sectors saved: {sector_count}\")\n",
    "    \n",
    "    return main_output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stock_Index_fundation_file = \"\" ### Path of your stocks' fundamental data\n",
    "Stock_Index_price_file = \"\" ### Path of your stocks' daily performance data\n",
    "output_dir = \"\" ### Where do you like to output your results\n",
    "include_sector0 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"S&P 500 Fundamental Data Preprocessing Tool\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Fundamental file: {Stock_Index_fundation_file}\")\n",
    "print(f\"Price file: {Stock_Index_price_file}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Include sector 0 in sector files: {include_sector0}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Load data\n",
    "fund_df, df_daily_price = load_data(Stock_Index_fundation_file, Stock_Index_price_file)\n",
    "\n",
    "# Process data\n",
    "fund_df = adjust_trade_dates(fund_df)\n",
    "fund_df = calculate_adjusted_close(fund_df)\n",
    "fund_df = match_tickers_and_gvkey(fund_df, df_daily_price)\n",
    "fund_df = calculate_next_quarter_returns(fund_df)\n",
    "fund_df = calculate_basic_ratios(fund_df)\n",
    "\n",
    "# Select and process columns\n",
    "fund_data = select_columns(fund_df)\n",
    "\n",
    "# Calculate financial ratios\n",
    "ratios = calculate_financial_ratios(fund_data)\n",
    "\n",
    "# Handle missing values\n",
    "final_ratios = handle_missing_values(ratios)\n",
    "\n",
    "# Save results\n",
    "output_file = save_results(final_ratios, output_dir, include_sector0)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Processing completed successfully!\")\n",
    "print(f\"Final dataset shape: {final_ratios.shape}\")\n",
    "print(f\"Output saved to: {output_file}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stock Selection\n",
    "\n",
    "In this part, we will use processed fundamental data to predict stocks' future return.\n",
    "\n",
    "To be specific, we will use 3 different machine learning algorithms (Random Forest, LightGBM, XGBoost) to predict next quarter's performance. Instead of setting a specific date to calculate all the future returns, here we optimized to predict dynamically. While there is a new report published, we can immediately renew its latest prediction. Then, we choose stocks with top k% (you can adjust this rate by yourself) predicted return as our portfolio.\n",
    "\n",
    "For details, please refer to `fundamental_run_model.py` and `ml_model.py`. Also, we recommend you to try DRL stock selection and reallocation algorithm in [AI4Finance/FinRL-Trading/fundamental_portfolio_drl.py](https://github.com/AI4Finance-Foundation/FinRL-Trading/blob/master_backup/fundamental_portfolio_drl.py).\n",
    "\n",
    "After running this part, you should get a csv file contains all your portfolio's components history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory_if_not_exists(directory_path):\n",
    "    \"\"\"\n",
    "    Create directory if it doesn't exist\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Directory path\n",
    "    \"\"\"\n",
    "    path = Path(directory_path)\n",
    "    if not path.exists():\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"✓ Created directory: {directory_path}\")\n",
    "    else:\n",
    "        print(f\"✓ Directory already exists: {directory_path}\")\n",
    "\n",
    "def quarter_ffill(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    qkey = df.index.to_period(\"Q\")\n",
    "    df = df.groupby(qkey).ffill()\n",
    "    return df\n",
    "\n",
    "def quarter_reportday_ffill(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    full_idx = pd.bdate_range(df.index.min(), df.index.max())\n",
    "\n",
    "    out = df.reindex(full_idx)\n",
    "    out = out.sort_index().copy()\n",
    "    idx = out.index\n",
    "    for col in out.columns:\n",
    "        s = out[col]\n",
    "        if not s.notna().any():\n",
    "            continue\n",
    "        dates = s.dropna().index\n",
    "        for i, t0 in enumerate(dates):\n",
    "            val = s.at[t0]\n",
    "            t1 = dates[i + 1] if i + 1 < len(dates) else None\n",
    "            cap = (t0.to_period('Q') + 1).end_time.normalize()\n",
    "            mask = (idx > t0) & (idx < (t1 if t1 and t1 <= cap else cap + pd.Timedelta('1ns')))\n",
    "            out.loc[mask, col] = val\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stock_selection(data_path, output_path):\n",
    "    \"\"\"\n",
    "    Run stock selection model\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Input sector files directory\n",
    "        output_path (str): Output directory\n",
    "    \"\"\"\n",
    "    # Define sector range\n",
    "    sectors = range(10, 65, 5)\n",
    "    \n",
    "    # Set data directory path\n",
    "    DATA_DIR = data_path\n",
    "    FUNDAMENTAL_FILE = os.path.join(DATA_DIR, \"final_ratios.csv\")\n",
    "    \n",
    "    print(f\"Using data directory: {DATA_DIR}\")\n",
    "    print(f\"Fundamental data file: {FUNDAMENTAL_FILE}\")\n",
    "    print(f\"Output directory: {output_path}\")\n",
    "    print(f\"Sector range: {list(sectors)}\")\n",
    "    \n",
    "    # Check if input file exists\n",
    "    if not os.path.exists(FUNDAMENTAL_FILE):\n",
    "        print(f\"Error: Fundamental data file does not exist: {FUNDAMENTAL_FILE}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Create output directory\n",
    "    create_directory_if_not_exists(output_path)\n",
    "    \n",
    "    # gvkey is unique identifier\n",
    "    df_dict = {'tic': [], 'predicted_return': [], 'trade_date': []}\n",
    "    \n",
    "    # ===== Run stock selection for all sectors in my_outputs directory =====\n",
    "    start = time.time()\n",
    "    print(\"\\nStarting stock selection model for all sectors...\")\n",
    "\n",
    "    df_all = pd.DataFrame()\n",
    "    \n",
    "    for sector in sectors:\n",
    "        sector_file = os.path.join(DATA_DIR, f\"sector{sector}.xlsx\")\n",
    "        print(f\"\\nProcessing sector{sector}...\")\n",
    "        \n",
    "        # Check if sector file exists\n",
    "        if not os.path.exists(sector_file):\n",
    "            print(f\"   Warning: Sector file does not exist, skipping: {sector_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Run model training - using files from specified directory\n",
    "        cmd = f\"{sys.executable} fundamental_run_model.py -sector_name sector{sector} -tic_column tic -fundamental {FUNDAMENTAL_FILE} -sector {sector_file}\"\n",
    "        print(f\"Executing command: {cmd}\")\n",
    "        \n",
    "        result = os.system(cmd)\n",
    "        if result != 0:\n",
    "            print(f\" sector{sector} model training failed\")\n",
    "            continue\n",
    "        \n",
    "        # Read prediction results\n",
    "        result_file = f\"results/sector{sector}/df_predict_best.csv\"\n",
    "        if not os.path.exists(result_file):\n",
    "            print(f\" Prediction result file does not exist: {result_file}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(result_file, index_col=0)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        print(f\"  Reading prediction results: {df.shape[0]} dates, {df.shape[1]} stocks\")\n",
    "        df = quarter_reportday_ffill(df)\n",
    "        df_all = pd.concat([df_all, df], axis=1)\n",
    "        print(f\" sector{sector} processing completed\")\n",
    "\n",
    "    for idx in df_all.index:\n",
    "        predicted_return = df_all.loc[idx]\n",
    "        ### Choose top 25% stocks, or you can adjust by yourself\n",
    "        btm_q = predicted_return.quantile(0.75)\n",
    "        # top_q = predicted_return.quantile(1)\n",
    "        predicted_return = predicted_return[predicted_return >= btm_q]\n",
    "        # predicted_return = predicted_return[(predicted_return >= btm_q) & (predicted_return <= top_q)]\n",
    "        for tic in predicted_return.index:\n",
    "            df_dict[\"tic\"].append(tic)\n",
    "            df_dict[\"predicted_return\"].append(predicted_return[tic])\n",
    "            df_dict[\"trade_date\"].append(idx)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print(f\"\\nTotal time: {(end-start)/60:.2f} minutes\")\n",
    "    print(f\"Processing completed! Total records: {len(df_dict['tic'])}\")\n",
    "    \n",
    "    # Create result DataFrame\n",
    "    df_result = pd.DataFrame(df_dict)\n",
    "    \n",
    "    # Save results to CSV file\n",
    "    output_file = os.path.join(output_path, \"stock_selected.csv\")\n",
    "    df_result.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to: {output_file}\")\n",
    "    \n",
    "    # Display result statistics\n",
    "    if len(df_result) > 0:\n",
    "        print(f\"\\nResult statistics:\")\n",
    "        print(f\"  Total records: {len(df_result)}\")\n",
    "        print(f\"  Unique stocks: {df_result['tic'].nunique()}\")\n",
    "        print(f\"  Date range: {df_result['trade_date'].min()} to {df_result['trade_date'].max()}\")\n",
    "        print(f\"  Predicted return range: {df_result['predicted_return'].min():.4f} to {df_result['predicted_return'].max():.4f}\")\n",
    "    else:\n",
    "        print(\"\\nWarning: No stock selection results generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = output_dir ### Path of your fundamental data's, should be the same of your choice in part 1\n",
    "output_path_step2 = \"\" ### Where do you want to output the result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_stock_selection(data_path, output_path_step2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Backtest\n",
    "\n",
    "In this part, you should use your result from part 2 to design a quantitative trading strategy. Your portfolio's performance should **beat S&P 500** during all the time.\n",
    "\n",
    "Here we provide you with a simple strategy: equal weight portfolio with buy & hold strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ Equal Weight Portfolio Construction ============\n",
    "print(\"=\" * 80)\n",
    "print(\"Equal Weight Portfolio Construction\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 1: Load selected stocks data\n",
    "selected_stocks_df = pd.read_csv(os.path.join(output_path_step2, \"stock_selected.csv\"))\n",
    "selected_stocks_df['trade_date'] = pd.to_datetime(selected_stocks_df['trade_date'])\n",
    "\n",
    "print(f\"\\nSelected stocks data shape: {selected_stocks_df.shape}\")\n",
    "print(f\"Date range: {selected_stocks_df['trade_date'].min()} to {selected_stocks_df['trade_date'].max()}\")\n",
    "print(f\"Unique stocks: {selected_stocks_df['tic'].nunique()}\")\n",
    "print(f\"\\nStock list: {sorted(selected_stocks_df['tic'].unique())}\")\n",
    "\n",
    "# Step 2: Load daily price data\n",
    "price_columns = ['tic', 'datadate', 'prccd']\n",
    "df_daily_price = pd.read_csv(Stock_Index_price_file, usecols=price_columns)\n",
    "df_daily_price['datadate'] = pd.to_datetime(df_daily_price['datadate'])\n",
    "df_daily_price = df_daily_price.rename(columns={'prccd': 'close'})\n",
    "\n",
    "print(f\"\\nDaily price data shape: {df_daily_price.shape}\")\n",
    "print(f\"Price date range: {df_daily_price['datadate'].min()} to {df_daily_price['datadate'].max()}\")\n",
    "\n",
    "# Step 3: Create portfolio weights dataframe\n",
    "portfolio_dates = sorted(selected_stocks_df['trade_date'].unique())\n",
    "all_stocks_in_portfolio = sorted(selected_stocks_df['tic'].unique())\n",
    "\n",
    "print(f\"\\nPortfolio dates: {len(portfolio_dates)}\")\n",
    "print(f\"All stocks in portfolio: {len(all_stocks_in_portfolio)}\")\n",
    "\n",
    "portfolio_weights_dict = {}\n",
    "\n",
    "for date in portfolio_dates:\n",
    "    stocks_on_date = selected_stocks_df[selected_stocks_df['trade_date'] == date]['tic'].unique()\n",
    "    n_stocks = len(stocks_on_date)\n",
    "    \n",
    "    portfolio_weights_dict[date] = {}\n",
    "    for stock in all_stocks_in_portfolio:\n",
    "        if stock in stocks_on_date:\n",
    "            portfolio_weights_dict[date][stock] = 1.0 / n_stocks\n",
    "        else:\n",
    "            portfolio_weights_dict[date][stock] = 0.0\n",
    "\n",
    "portfolio_weights_df = pd.DataFrame(portfolio_weights_dict).T\n",
    "portfolio_weights_df.index.name = 'date'\n",
    "\n",
    "print(f\"\\nPortfolio weights dataframe shape: {portfolio_weights_df.shape}\")\n",
    "print(f\"Rows (dates): {portfolio_weights_df.shape[0]}, Columns (stocks): {portfolio_weights_df.shape[1]}\")\n",
    "print(f\"\\nFirst few dates and weights:\")\n",
    "print(portfolio_weights_df.head(10))\n",
    "print(f\"\\nLast few dates and weights:\")\n",
    "print(portfolio_weights_df.tail(10))\n",
    "\n",
    "weights_sum = portfolio_weights_df.sum(axis=1)\n",
    "print(f\"\\nWeights sum check (should all be 1.0):\")\n",
    "print(f\"  Min: {weights_sum.min():.6f}\")\n",
    "print(f\"  Max: {weights_sum.max():.6f}\")\n",
    "print(f\"  Mean: {weights_sum.mean():.6f}\")\n",
    "\n",
    "# Step 4: Save the portfolio weights\n",
    "portfolio_weights_file = os.path.join(output_path_step2, \"portfolio_weights.csv\")\n",
    "portfolio_weights_df.to_csv(portfolio_weights_file)\n",
    "print(f\"\\n✓ Portfolio weights saved to: {portfolio_weights_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Equal weight portfolio constructed successfully!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = pd.read_csv(\"daily.csv\", parse_dates=[\"datadate\"])\n",
    "\n",
    "daily = daily.rename(columns={\n",
    "    \"datadate\": \"date\",\n",
    "    \"prcod\": \"open\",\n",
    "    \"prccd\": \"close\"\n",
    "})\n",
    "\n",
    "daily[\"open_adj\"] = daily[\"open\"] / daily[\"ajexdi\"]\n",
    "daily[\"close_adj\"] = daily[\"close\"] / daily[\"ajexdi\"]\n",
    "\n",
    "daily[\"ret\"] = daily[\"close_adj\"] / daily[\"open_adj\"] - 1\n",
    "\n",
    "ret_df = (\n",
    "    daily\n",
    "    .pivot(index=\"date\", columns=\"tic\", values=\"ret\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "portfolio_weights_df = portfolio_weights_df.sort_index()\n",
    "\n",
    "common_dates = portfolio_weights_df.index.intersection(ret_df.index)\n",
    "portfolio_weights_df = portfolio_weights_df.loc[common_dates]\n",
    "ret_df = ret_df.loc[common_dates]\n",
    "\n",
    "ret_df = ret_df.fillna(0.0)\n",
    "\n",
    "\n",
    "gross_ret = (portfolio_weights_df * ret_df).sum(axis=1)\n",
    "turnover = portfolio_weights_df.diff().abs().sum(axis=1)\n",
    "\n",
    "fee_rate = 0.001  # 0.1%\n",
    "cost = fee_rate * turnover\n",
    "cost.iloc[0] = 0.0\n",
    "\n",
    "net_ret = gross_ret - cost\n",
    "nav = (1 + net_ret).cumprod()\n",
    "\n",
    "result = pd.DataFrame({\n",
    "    \"gross_ret\": gross_ret,\n",
    "    \"turnover\": turnover,\n",
    "    \"cost\": cost,\n",
    "    \"net_ret\": net_ret,\n",
    "    \"nav\": nav\n",
    "})\n",
    "\n",
    "initial_date = portfolio_weights_df.index[0] - pd.Timedelta(days=1)\n",
    "initial_row = pd.DataFrame({\n",
    "    \"gross_ret\": [0.0],\n",
    "    \"turnover\": [0.0],\n",
    "    \"cost\": [0.0],\n",
    "    \"net_ret\": [0.0],\n",
    "    \"nav\": [1.0]\n",
    "}, index=[initial_date])\n",
    "\n",
    "result = pd.concat([initial_row, result], axis=0)\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "print(f\"First few rows:\\n{result.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QQQ = yf.download(\"QQQ\", start=result.index.min(), end=result.index.max(), progress=False, multi_level_index=False)\n",
    "spy = yf.download(\"SPY\", start=result.index.min(), end=result.index.max(), progress=False, multi_level_index=False)\n",
    "\n",
    "qqq_nav = QQQ['Close'] / QQQ['Close'].iloc[0]\n",
    "spy_nav = spy['Close'] / spy['Close'].iloc[0]\n",
    "\n",
    "# Create a dataframe with all NAVs\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Portfolio': result['nav'],\n",
    "    'QQQ': qqq_nav,\n",
    "    'SPY': spy_nav\n",
    "})\n",
    "\n",
    "# Forward fill to handle missing dates\n",
    "comparison_df = comparison_df.fillna(method='ffill')\n",
    "\n",
    "# Remove rows with NaN\n",
    "comparison_df = comparison_df.dropna()\n",
    "\n",
    "# Normalize all NAV curves to start from 1.0\n",
    "print(\"Normalizing NAV curves to start from 1.0...\")\n",
    "comparison_df = comparison_df / comparison_df.iloc[0]\n",
    "\n",
    "print(f\"Aligned data shape: {comparison_df.shape}\")\n",
    "print(f\"Date range: {comparison_df.index.min()} to {comparison_df.index.max()}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plot NAV curves\n",
    "ax.plot(comparison_df.index, comparison_df['Portfolio'], label='My Portfolio', linewidth=2.5, color='#1f77b4')\n",
    "ax.plot(comparison_df.index, comparison_df['QQQ'], label='QQQ (NASDAQ-100)', linewidth=2, color='#ff7f0e', alpha=0.8)\n",
    "ax.plot(comparison_df.index, comparison_df['SPY'], label='SPY (S&P 500)', linewidth=2, color='#2ca02c', alpha=0.8)\n",
    "\n",
    "# Add horizontal line at 1.0\n",
    "ax.axhline(y=1.0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel('Date', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Net Asset Value (NAV)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Portfolio Performance Comparison: My Portfolio vs QQQ vs S&P 500 (Normalized to 1.0)', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(fontsize=11, loc='best', framealpha=0.95)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Format x-axis\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Format y-axis\n",
    "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.2f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "output_file = 'portfolio_comparison_chart.png'\n",
    "plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"Chart saved to: {output_file}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# Your own strategy and visulization can be built based on the above code #\n",
    "###########################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
