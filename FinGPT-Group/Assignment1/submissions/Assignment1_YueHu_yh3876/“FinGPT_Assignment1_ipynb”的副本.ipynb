{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "07b7ab67",
      "metadata": {
        "id": "07b7ab67"
      },
      "source": [
        "# GR5398 26Spring: FinGPT Large Language Model Track\n",
        "## Assignment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6db642b",
      "metadata": {
        "id": "b6db642b"
      },
      "source": [
        "### 1. Data Preparation\n",
        "\n",
        "In this part, I generate dataset from Dow Jones 30's component stocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e305c2e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e305c2e7",
        "outputId": "cc5300ff-2a58-4dd1-dd15-de72b166d4cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.5.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.24.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.10.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.24.2)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (23.0.1)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (0.24.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (0.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (0.1.2)\n",
            "Requirement already satisfied: finnhub-python in /usr/local/lib/python3.12/dist-packages (2.4.27)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.12/dist-packages (from finnhub-python) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->finnhub-python) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->finnhub-python) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->finnhub-python) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->finnhub-python) (2026.1.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers accelerate peft datasets\n",
        "!pip install finnhub-python\n",
        "!pip install -U bitsandbytes>=0.46.1\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import math\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import finnhub\n",
        "import datasets\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "from datasets import Dataset\n",
        "from openai import OpenAI\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig # Added BitsAndBytesConfig import\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dow Jones 30 dataset provided by FinGPT (May 2023 - May 2024)\n",
        "splits = {\n",
        "    \"train\": \"data/train-00000-of-00001-7c4c80aa07272d4c.parquet\",\n",
        "    \"test\": \"data/test-00000-of-00001-28531804b005ddc6.parquet\"\n",
        "}\n",
        "\n",
        "train_df = pd.read_parquet(\n",
        "    \"hf://datasets/FinGPT/fingpt-forecaster-dow30-202305-202405/\"\n",
        "    + splits[\"train\"]\n",
        ")\n",
        "\n",
        "test_df = pd.read_parquet(\n",
        "    \"hf://datasets/FinGPT/fingpt-forecaster-dow30-202305-202405/\"\n",
        "    + splits[\"test\"]\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(train_df))\n",
        "print(\"Test size:\", len(test_df))\n",
        "train_df.head()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "UDSwqN0xkglb",
        "outputId": "cc679bbb-fbbb-4647-c33b-8dc0369ffb47"
      },
      "id": "UDSwqN0xkglb",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 1230\n",
            "Test size: 300\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              prompt  \\\n",
              "0  [INST]<<SYS>>\\nYou are a seasoned stock market...   \n",
              "1  [INST]<<SYS>>\\nYou are a seasoned stock market...   \n",
              "2  [INST]<<SYS>>\\nYou are a seasoned stock market...   \n",
              "3  [INST]<<SYS>>\\nYou are a seasoned stock market...   \n",
              "4  [INST]<<SYS>>\\nYou are a seasoned stock market...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  [Positive Developments]:\\n1. Market Outperform...   \n",
              "1  [Positive Developments]:\\n1. American Express'...   \n",
              "2  [Positive Developments]:\\n1. Increased dividen...   \n",
              "3  [Positive Developments]:\\n1. The stock outperf...   \n",
              "4  [Positive Developments]:\\n1. American Express ...   \n",
              "\n",
              "                     period               label symbol  \n",
              "0  2023-05-14 to 2023-05-21          up by 3-4%    AXP  \n",
              "1  2023-05-21 to 2023-05-28          up by 2-3%    AXP  \n",
              "2  2023-05-28 to 2023-06-04  up by more than 5%    AXP  \n",
              "3  2023-06-04 to 2023-06-11          up by 1-2%    AXP  \n",
              "4  2023-06-11 to 2023-06-18          up by 0-1%    AXP  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4b5df369-bb2b-4fc2-afa8-5f9902d64a63\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>answer</th>\n",
              "      <th>period</th>\n",
              "      <th>label</th>\n",
              "      <th>symbol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[INST]&lt;&lt;SYS&gt;&gt;\\nYou are a seasoned stock market...</td>\n",
              "      <td>[Positive Developments]:\\n1. Market Outperform...</td>\n",
              "      <td>2023-05-14 to 2023-05-21</td>\n",
              "      <td>up by 3-4%</td>\n",
              "      <td>AXP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[INST]&lt;&lt;SYS&gt;&gt;\\nYou are a seasoned stock market...</td>\n",
              "      <td>[Positive Developments]:\\n1. American Express'...</td>\n",
              "      <td>2023-05-21 to 2023-05-28</td>\n",
              "      <td>up by 2-3%</td>\n",
              "      <td>AXP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[INST]&lt;&lt;SYS&gt;&gt;\\nYou are a seasoned stock market...</td>\n",
              "      <td>[Positive Developments]:\\n1. Increased dividen...</td>\n",
              "      <td>2023-05-28 to 2023-06-04</td>\n",
              "      <td>up by more than 5%</td>\n",
              "      <td>AXP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[INST]&lt;&lt;SYS&gt;&gt;\\nYou are a seasoned stock market...</td>\n",
              "      <td>[Positive Developments]:\\n1. The stock outperf...</td>\n",
              "      <td>2023-06-04 to 2023-06-11</td>\n",
              "      <td>up by 1-2%</td>\n",
              "      <td>AXP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[INST]&lt;&lt;SYS&gt;&gt;\\nYou are a seasoned stock market...</td>\n",
              "      <td>[Positive Developments]:\\n1. American Express ...</td>\n",
              "      <td>2023-06-11 to 2023-06-18</td>\n",
              "      <td>up by 0-1%</td>\n",
              "      <td>AXP</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4b5df369-bb2b-4fc2-afa8-5f9902d64a63')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4b5df369-bb2b-4fc2-afa8-5f9902d64a63 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4b5df369-bb2b-4fc2-afa8-5f9902d64a63');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 1230,\n  \"fields\": [\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1230,\n        \"samples\": [\n          \"[INST]<<SYS>>\\nYou are a seasoned stock market analyst. Your task is to list the positive developments and potential concerns for companies based on relevant news and basic financials from the past weeks, then provide an analysis and prediction for the companies' stock price movement for the upcoming week. Your answer format should be as follows:\\n\\n[Positive Developments]:\\n1. ...\\n\\n[Potential Concerns]:\\n1. ...\\n\\n[Prediction & Analysis]\\nPrediction: ...\\nAnalysis: ...\\n\\n<</SYS>>\\n\\n[Company Introduction]:\\n\\nCoca-Cola Co is a leading entity in the Beverages sector. Incorporated and publicly traded since 1950-01-26, the company has established its reputation as one of the key players in the market. As of today, Coca-Cola Co has a market capitalization of 266302.24 in USD, with 4311.19 shares outstanding.\\n\\nCoca-Cola Co operates primarily in the US, trading under the ticker KO on the NEW YORK STOCK EXCHANGE, INC.. As a dominant force in the Beverages space, the company continues to innovate and drive progress within the industry.\\n\\nFrom 2023-08-13 to 2023-08-20, KO's stock price decreased from 59.73 to 59.52. News during this period are listed below:\\n\\n[Headline]: CocaCola Company (The) (KO) Is a Trending Stock: Facts to Know Before Betting on It\\n[Summary]: Coke (KO) has been one of the stocks most watched by Zacks.com users lately. So, it is worth exploring what lies ahead for the stock.\\n\\n[Headline]: Can Coca-Cola Continue Maximizing Dividends While Balancing Growth?\\n[Summary]: Coca-Cola (NYSE: KO) has been a true dividend stalwart, providing an incredible 61 years of consecutive dividend increases -- and understandably, it continues to capture the attention of income-focused investors.  Coca-Cola's reputation as a dependable dividend payer draws investors seeking stable income over time, who want to hold a stock that delivers payments on a regular basis instead of banking on expansion.  Recent financial performance adds another layer of confidence, with net revenue experiencing a robust 6% growth, reaching an impressive $12 billion, and organic revenue surging by a remarkable 11%, showcasing the company's robust financial standing.\\n\\n[Headline]: 2 Stocks Under $100 You Can Buy and Hold Forever\\n[Summary]: Dividend stocks are an excellent way to weather stock market volatility.  Among the most impressive are Dividend Kings -- S&P 500 companies that have increased their dividends for at least 50 consecutive years, demonstrating the stability of their operations.  Here are two Dividend Kings that you can invest in for less than $100.\\n\\n[Headline]: Cramer's Lightning Round: Hold DraftKings\\n[Summary]: \\\"Mad Money\\\" host Jim Cramer rings the lightning round bell, which means he's giving his answers to callers' stock questions at rapid speed.\\n\\n[Headline]: Warren Buffett-Led Berkshire Hathaway Sells 9.29 Million Shares of Chevron Stock. What Now?\\n[Summary]: On Aug. 14, Warren Buffett-led Berkshire Hathaway (NYSE: BRK.A) (NYSE: BRK.B) released its quarterly 13F filing, which shows holdings as of June 30. Five companies make up 79.6% of Berkshire's public equity portfolio.\\n\\nFrom 2023-08-20 to 2023-08-27, KO's stock price decreased from 59.52 to 58.97. News during this period are listed below:\\n\\n[Headline]: The 7 Most Undervalued Dividend Stocks to Buy Now: August 2023\\n[Summary]: Generating passive income is easier said than done. A lot of investors look for dividend stocks to enjoy steady and consistent income. However, you need to be careful when investing in dividend stocks. While the dividend yield provides an idea about the payout, it is important to look for companies that show steady dividend growth and have a stable balance sheet. When you look for stable companies, the dividends are more stable and you can also enjoy capital appreciation over time. Here are seve\\n\\n[Headline]: Tracking Ray Dalio's Bridgewater Associates 13F Portfolio - Q2 2023 Update\\n[Summary]: Bridgewater Associates' 13F portfolio value decreased by approximately 1% in Q2 2023. Click here to read the full Q2 2023 update.\\n\\n[Headline]: Tracking Jeremy Grantham's GMO Capital Portfolio - Q2 2023 Update\\n[Summary]: Jeremy Grantham's 13F portfolio value increased by 10% this quarter, reaching $21.60B. Check out GMO Capital's holdings and trades for Q2 2023.\\n\\n[Headline]: Coca Cola : Investor Overview \\u2013 Updated for Second Quarter 2023\\n[Summary]: The Coca-Cola Company INVESTOR OVERVIEW UPDATED FOR SECOND QUARTER 2023 ...\\n\\n[Headline]: The Buffett List: 6 Dividend Buys In August\\n[Summary]: Analyst-augured top-ten net gains from Buffett\\u00e2\\u0080\\u0099s August dividend dogs ranged 19.12%-56.48% from CVX, COF, DHI, LPX, KHC, BAC, PARA, ALLY, C, and top pick GM per YCharts data. Read more here.\\n\\nSome recent basic financials of KO, reported at 2023-06-30, are presented below:\\n\\n[Basic Financials]:\\n\\nassetTurnoverTTM: 0.4633\\nbookValue: 26013\\ncashRatio: 0.5210035247771097\\ncurrentRatio: 1.1441\\nebitPerShare: 0.5531\\neps: 0.5867\\nev: 289452.28\\nfcfMargin: 0.345\\nfcfPerShareTTM: 2.1945\\ngrossMargin: 0.5897\\ninventoryTurnoverTTM: 4.4292\\nlongtermDebtTotalAsset: 0.3618\\nlongtermDebtTotalCapital: 0.5267\\nlongtermDebtTotalEquity: 1.3695\\nnetDebtToTotalCapital: 0.4297\\nnetDebtToTotalEquity: 1.1172\\nnetMargin: 0.2127\\noperatingMargin: 0.2006\\npayoutRatioTTM: 0.5609\\npb: 10.01\\npeTTM: 24.7756\\npfcfTTM: 27.11\\npretaxMargin: 0.2406\\npsTTM: 5.8992\\nptbv: 23.518\\nquickRatio: 0.8154\\nreceivablesTurnoverTTM: 10.4301\\nroaTTM: 0.1103\\nroeTTM: 0.4284\\nroicTTM: 0.1611\\nrotcTTM: 0.1765\\nsalesPerShare: 2.7579\\nsgaToSale: 0.4103\\ntangibleBookValue: 11072\\ntotalDebtToEquity: 1.6002\\ntotalDebtToTotalAsset: 0.4228\\ntotalDebtToTotalCapital: 0.6154\\ntotalRatio: 1.3591\\n\\nBased on all the information before 2023-08-27, let's first analyze the positive developments and potential concerns for KO. Come up with 2-4 most important factors respectively and keep them concise. Most factors should be inferred from company related news. Then make your prediction of the KO cryptocurrency price movement for next week (2023-08-27 to 2023-09-03). Provide a summary analysis to support your prediction.[/INST]\",\n          \"[INST]<<SYS>>\\nYou are a seasoned stock market analyst. Your task is to list the positive developments and potential concerns for companies based on relevant news and basic financials from the past weeks, then provide an analysis and prediction for the companies' stock price movement for the upcoming week. Your answer format should be as follows:\\n\\n[Positive Developments]:\\n1. ...\\n\\n[Potential Concerns]:\\n1. ...\\n\\n[Prediction & Analysis]\\nPrediction: ...\\nAnalysis: ...\\n\\n<</SYS>>\\n\\n[Company Introduction]:\\n\\nMerck & Co Inc is a leading entity in the Pharmaceuticals sector. Incorporated and publicly traded since 1941-01-01, the company has established its reputation as one of the key players in the market. As of today, Merck & Co Inc has a market capitalization of 327268.22 in USD, with 2533.03 shares outstanding.\\n\\nMerck & Co Inc operates primarily in the US, trading under the ticker MRK on the NEW YORK STOCK EXCHANGE, INC.. As a dominant force in the Pharmaceuticals space, the company continues to innovate and drive progress within the industry.\\n\\nFrom 2023-06-18 to 2023-06-25, MRK's stock price increased from 107.12 to 112.29. News during this period are listed below:\\n\\n[Headline]: Drugmakers aim to strike down Medicare drug-price negotiations at Supreme Court \\n[Summary]: The lawsuits are the opening salvo in a historic and potentially decisive battle over Medicare's efforts to bring rising drug prices under control. \\n\\n[Headline]: Merck & Co. Inc. stock rises Tuesday, outperforms market\\n[Summary]: Shares of Merck & Co. Inc. inched 0.83% higher to $110.23 Tuesday, on what proved to be an all-around dismal trading session for the stock market, with the...\\n\\n[Headline]: Merck Says Keytruda Regimen Fails To Improve Event-Free Survival In Gastric Cancer Patients\\n[Summary]: Merck & Co Inc (NYSE: MRK) announced topline results from the Phase 3 KEYNOTE-585 trial of Keytruda (pembrolizumab) in combination with chemotherapy as a neoadjuvant treatment regime. The Keytruda + chemotherapy regime was followed by adjuvant treatment with Keytruda plus chemotherapy, then Keytruda monotherapy in patients with locally advanced resectable gastric and gastroesophageal junction (GEJ) adenocarcinoma. Also Read: Merck's Keytruda Combo Therapy Hits Primary Goal In Patients With Rare\\n\\n[Headline]: Merck's (MRK) Gastric Cancer Study Fails to Meet a Primary Goal\\n[Summary]: Merck's (MRK) late-stage study evaluating Keytruda in advanced gastric cancer fails to meet one of the primary endpoints.\\n\\n[Headline]: UPDATE 2-Pharmaceutical trade group sues US over Medicare drug price negotiation plans\\n[Summary]: The Pharmaceutical Research and Manufacturers of America (PhRMA), the leading industry lobby group, and two other organizations on Wednesday said they were suing the U.S. government to block enforcement of a program that gives Medicare the power to negotiate drug prices.  In a complaint filed in a federal court in Texas, PhRMA along with the National Infusion Center Association and the Global Colon Cancer Association, which counts PhRMA and some drug companies as members, said the drug price negotiation program was unconstitutional.  This marks the fourth lawsuit challenging the law, which is part of President Joe Biden\\u2019s signature Inflation Reduction Act (IRA), after separate legal challenges by Merck & Co, Bristol Myers Squibb, and the influential business group the U.S. Chamber of Commerce.\\n\\nFrom 2023-06-25 to 2023-07-02, MRK's stock price increased from 112.29 to 113.07. News during this period are listed below:\\n\\n[Headline]: Top 16 Medicine Producing Countries in the World\\n[Summary]: In this article, we will be covering the top 16 medicine producing countries in the world. If you want to skip our detailed analysis of the pharmaceutical industry and emerging trends, go directly to the Top 5 Medicine Producing Countries in the World. The global pharmaceutical industry is forecasted to grow from $1.4 billion in [\\u2026]\\n\\n[Headline]: 10 Dow Stocks Billionaires Are Loading Up On\\n[Summary]: In this article, we will take a look at the 10 Dow stocks billionaires are loading up on. To see more such companies, go directly to 5 Dow Stocks Billionaires Are Loading Up On. In Tony Robbins\\u2019 Money \\u2013 Master The Game, there\\u2019s a chapter titled Invest Like the .001%: The Billionaire\\u2019s Playbook. The chapter mentions the [\\u2026]\\n\\n[Headline]: Antiquated War Exclusion Does Not Bar Coverage For $700 Million Cyber Insurance Claim\\n[Summary]: Does a devastating computer virus attack, that might be sponsored by a nation state, qualify as \\n\\n[Headline]: Merck & Co. Inc. stock outperforms competitors on strong trading day\\n[Summary]: Shares of Merck & Co. Inc. inched 0.98% higher to $113.54 Thursday, on what proved to be an all-around positive trading session for the stock market, with...\\n\\n[Headline]: Medicare will allow pharmaceutical companies to publicly discuss drug price negotiations\\n[Summary]: Drugmakers had sued the federal government, arguing that the \\\"gag order\\\" violated the First Amendment. \\n\\nSome recent basic financials of MRK, reported at 2023-06-30, are presented below:\\n\\n[Basic Financials]:\\n\\nassetTurnoverTTM: 0.5443\\nbookValue: 38693\\ncashRatio: 0.24194237838762075\\ncurrentRatio: 1.2814\\nebitPerShare: -2.0335\\neps: -2.3533\\nev: 324045.75\\nfcfMargin: 0.1822\\nfcfPerShareTTM: 4.264\\ngrossMargin: 0.7345\\ninventoryTurnoverTTM: 2.7234\\nlongtermDebtTotalAsset: 0.3261\\nlongtermDebtTotalCapital: 0.4507\\nlongtermDebtTotalEquity: 0.8806\\nnetDebtToTotalCapital: 0.4134\\nnetDebtToTotalEquity: 0.8077\\nnetMargin: -0.3974\\noperatingMargin: -0.3434\\npb: 7.5671\\npeTTM: 94.116\\npfcfTTM: 27.0405\\npretaxMargin: -0.3548\\npsTTM: 5.0213\\nptbv: 15.3876\\nquickRatio: 1.0279\\nreceivablesTurnoverTTM: 5.6413\\nroaTTM: 0.029\\nroeTTM: 0.0707\\nroicTTM: 0.0408\\nrotcTTM: 0.0716\\nsalesPerShare: 5.9216\\nsgaToSale: 0.2655\\ntangibleBookValue: 19028\\ntotalDebtToEquity: 0.9539\\ntotalDebtToTotalAsset: 0.3533\\ntotalDebtToTotalCapital: 0.4882\\ntotalRatio: 1.5883\\n\\nBased on all the information before 2023-07-02, let's first analyze the positive developments and potential concerns for MRK. Come up with 2-4 most important factors respectively and keep them concise. Most factors should be inferred from company related news. Then make your prediction of the MRK cryptocurrency price movement for next week (2023-07-02 to 2023-07-09). Provide a summary analysis to support your prediction.[/INST]\",\n          \"[INST]<<SYS>>\\nYou are a seasoned stock market analyst. Your task is to list the positive developments and potential concerns for companies based on relevant news and basic financials from the past weeks, then provide an analysis and prediction for the companies' stock price movement for the upcoming week. Your answer format should be as follows:\\n\\n[Positive Developments]:\\n1. ...\\n\\n[Potential Concerns]:\\n1. ...\\n\\n[Prediction & Analysis]\\nPrediction: ...\\nAnalysis: ...\\n\\n<</SYS>>\\n\\n[Company Introduction]:\\n\\nCisco Systems Inc is a leading entity in the Communications sector. Incorporated and publicly traded since 1990-02-16, the company has established its reputation as one of the key players in the market. As of today, Cisco Systems Inc has a market capitalization of 190230.80 in USD, with 4049.19 shares outstanding.\\n\\nCisco Systems Inc operates primarily in the US, trading under the ticker CSCO on the NASDAQ NMS - GLOBAL MARKET. As a dominant force in the Communications space, the company continues to innovate and drive progress within the industry.\\n\\nFrom 2024-01-21 to 2024-01-28, CSCO's stock price increased from 50.85 to 51.72. News during this period are listed below:\\n\\n[Headline]: Dividend Champion, Contender, And Challenger Highlights: Week Of January 21\\n[Summary]: A weekly summary of dividend activity for Dividend Champions, Contenders, and Challengers. Find out the full update for the week of January 21.\\n\\n[Headline]: Cisco Systems Inc. stock underperforms Wednesday when compared to competitors\\n[Summary]: Shares of Cisco Systems Inc. sank 0.87% to $51.31 Wednesday, on what proved to be an all-around mixed trading session for the stock market, with the S&P 500...\\n\\n[Headline]: More than 1 in 4 Organizations Banned Use of GenAI Over Privacy and Data Security Risks - New Cisco Study\\n[Summary]: Cisco (NASDAQ: CSCO) today released its 2024 Data Privacy Benchmark Study, an annual review of key privacy issues and their impact on business. Ahead of International Data Privacy Day, the findings highlight the growing Privacy concerns with GenAI, trust challenges facing organizations over their use of AI, and the attractive returns from privacy investment. Drawing on responses from 2,600 privacy and security professionals across 12 geographies, the seventh edition of the Benchmark shows that p\\n\\n[Headline]: Dividends May Likely Crush Growth In 2024: SCHD Vs. SCHG\\n[Summary]: Low-cost ETFs are advantageous for long-term wealth accumulation due to low expense ratios, diversification, & flexibility. Read more on how SCHG and SCHD fit in this approach.\\n\\n[Headline]: Cisco Systems Inc. stock underperforms Friday when compared to competitors\\n[Summary]: Shares of Cisco Systems Inc. slumped 0.36% to $52.14 Friday, on what proved to be an all-around mixed trading session for the stock market, with the Dow...\\n\\nFrom 2024-01-28 to 2024-02-04, CSCO's stock price decreased from 51.72 to 49.77. News during this period are listed below:\\n\\n[Headline]: Cisco: Sluggish Growth Ahead, But Shares Priced For Pessimism\\n[Summary]: Despite its weak growth prospects, Cisco is undervalued and offers a high dividend yield, making it a potential buy. Read more on CSCO stock here.\\n\\n[Headline]: Splunk Trading At An Implied Yield Close To 9.95% While Risk Is Mitigated\\n[Summary]: Cisco Systems is acquiring cybersecurity firm Splunk for $30B, with expected high returns and a good chance of deal completion. Click here for my SPLK strategy.\\n\\n[Headline]: Cisco Collaborates with Microsoft and Samsung to Deliver Superior Meeting Room Experiences\\n[Summary]: Today at Integrated Systems Europe (ISE) 2024, Cisco (NASDAQ: CSCO), Microsoft Corp. (NASDAQ: MSFT) and Samsung Electronics Co. Ltd. (NASDAQ: SSNLF) announced new meeting room solutions to deliver enhanced collaboration experiences for hybrid meetings. With a collective vision to enable seamless and inclusive meetings for all, the companies unveiled integrated video collaboration solutions for Cisco Room Series. The solutions feature Front Row - an inclusive content layout for Microsoft Teams Ro\\n\\n[Headline]: Cisco Systems Inc. stock outperforms competitors despite losses on the day\\n[Summary]: Shares of Cisco Systems Inc. slipped 0.10% to $52.24 Tuesday, on what proved to be an all-around mixed trading session for the stock market, with the Dow...\\n\\n[Headline]: USMV: A Unique Low-Volatility Strategy\\n[Summary]: iShares MSCI USA Min Vol Factor ETF implements a low-volatility strategy using an optimized covariance matrix. Read more on USMV ETF here.\\n\\nFrom 2024-02-04 to 2024-02-11, CSCO's stock price decreased from 49.77 to 49.72. News during this period are listed below:\\n\\n[Headline]: 2023 - Looking Back At Our Year Of Progress\\n[Summary]: Including drips, we put $43,460.72 of capital into the market.\\n\\n[Headline]: Cisco Systems (CSCO) Falls More Steeply Than Broader Market: What Investors Need to Know\\n[Summary]: Cisco Systems (CSCO) reachead $49.51 at the closing of the latest trading day, reflecting a -1.34% change compared to its last close.\\n\\n[Headline]: Cisco Unveils New Innovations on the Cisco Observability Platform\\n[Summary]: CISCO LIVE EMEA -- Cisco (NASDAQ: CSCO) today announced a series of exciting new solutions - enriched by business context - on the Cisco Observability Platform. With applications acting as the front door for nearly every business - and delivering a flawless application experience a top priority for IT teams - the latest enhancements will help customers deliver secure and performant user and application experience.\\n\\n[Headline]: Cisco Systems Inc. stock underperforms Wednesday when compared to competitors\\n[Summary]: Shares of Cisco Systems Inc. shed 0.42% to $49.77 Wednesday, on what proved to be an all-around favorable trading session for the stock market, with the S&P...\\n\\n[Headline]: Cisco Systems Inc. stock rises Friday, still underperforms market\\n[Summary]: Shares of Cisco Systems Inc. inched 0.36% higher to $50.13 Friday, on what proved to be an all-around mixed trading session for the stock market, with the...\\n\\nSome recent basic financials of CSCO, reported at 2024-01-27, are presented below:\\n\\n[Basic Financials]:\\n\\nassetTurnoverTTM: 0.5733\\nbookValue: 46251\\ncashRatio: 0.4445560921850183\\ncurrentRatio: 1.3655\\nebitPerShare: 0.7601\\neps: 0.6467\\nev: 210369.14\\nfcfMargin: 0.0499\\nfcfPerShareTTM: 3.3474\\ngrossMargin: 0.6424\\ninventoryTurnoverTTM: 6.4511\\nlongtermDebtTotalAsset: 0.0659\\nlongtermDebtTotalCapital: 0.1153\\nlongtermDebtTotalEquity: 0.1442\\nnetDebtToTotalCapital: -0.0365\\nnetDebtToTotalEquity: -0.0456\\nnetMargin: 0.2059\\noperatingMargin: 0.242\\npayoutRatioTTM: 0.472\\npb: 4.594\\npeTTM: 15.8071\\npfcfTTM: 15.673\\npretaxMargin: 0.2471\\npsTTM: 3.7125\\nptbv: 4.767\\nquickRatio: 1.2615\\nreceivablesTurnoverTTM: 8.5929\\nroaTTM: 0.1346\\nroeTTM: 0.3019\\nroicTTM: 0.2511\\nrotcTTM: 0.2908\\nsalesPerShare: 3.1404\\nsgaToSale: 0.3576\\ntangibleBookValue: 44573\\ntotalDebtToEquity: 0.2509\\ntotalDebtToTotalAsset: 0.1147\\ntotalDebtToTotalCapital: 0.2006\\ntotalRatio: 1.8421\\n\\nBased on all the information before 2024-02-11, let's first analyze the positive developments and potential concerns for CSCO. Come up with 2-4 most important factors respectively and keep them concise. Most factors should be inferred from company related news. Then make your prediction of the CSCO cryptocurrency price movement for next week (2024-02-11 to 2024-02-18). Provide a summary analysis to support your prediction.[/INST]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1230,\n        \"samples\": [\n          \"[Positive Developments]:\\n1. KO showed impressive financial performance with net revenue experiencing a robust 6% growth, reaching $12 billion.\\n2. Coca-Cola's reputation as a dependable dividend payer draws investors seeking stable income over time, securing its position as a reliable investment option.\\n3. The company has been recognized as a \\\"Dividend King\\\", a distinction that increases its appeal to investors seeking long-term investment prospects.\\n\\n[Potential Concerns]:\\n1. The company's stock price has been on a downward trend for two consecutive weeks, which may signal the beginnings of negative market sentiment.\\n2. Coca-Cola's high total debt to equity ratio indicates that the company is using debt to finance its business operations, which can potentially expose the company to financial risks should market conditions turn unfavorable.\\n3. Warren Buffett-led Berkshire Hathaway's recent share sell-offs in other companies could be concerning if such behavior extends to Coca-Cola.\\n\\n[Prediction & Analysis]:\\nPrediction: Down by 1-2%\\nAnalysis: While Coca Cola has demonstrated its financial strength, robust returns, and consistent dividend payouts, short-term concerns like the general downward trend of its stock price and the company's relatively high debt pose barriers to immediate stock price growth. These factors may be temporary, but they impact investor sentiment in the short term and will likely contribute to a modest decrease in the company's stock price in the week from 2023-08-27 to 2023-09-03. The decrease might be between 1-2%. However, investors\\u2019 long-term confidence in Coca-Cola, courtesy of its solid financial performance and status as a reliable dividend payer, indicates a resilient future outlook. As such, this short-term downtrend does not necessarily reflect negatively on the company's overall future performance potential.\",\n          \"[Positive Developments]:\\n1. Merck & Co Inc's stock continues to grow despite a challenging market environment, with events such as rising 0.83% higher on a dismal trading session and 0.98% higher on a strong trading day. \\n2. The company is being recognized in influential financial analyses, appearing in articles like the top Dow stocks billionaires are loading up on, which can increase investor interest and confidence. \\n3. Medicare has allowed pharmaceutical companies to publicly discuss drug price negotiations, potentially easing pressure on the pharmaceutical sector's profitability and granting Merck & Co. better predictability with regard to revenues and costs.\\n\\n[Potential Concerns]:\\n1. Merck & Co Inc has faced a significant setback, with its Keytruda regimen failing to improve event-free survival in gastric cancer patients, as reported in two separate instances. This could hinder the company's growth and profitability, particularly if the drug was projected to be a significant source of future revenue. \\n2. Merck is involved in a potentially impactful lawsuit concerning Medicare's drug price negotiation plans. The outcome of these complex legal issues is uncertain, potentially posing significant risks to the company.\\n\\n[Prediction & Analysis]:\\nPrediction: Down by more than 5%\\nAnalysis: Given recent developments, a prediction for a more than 5% drop in Merck's stock price over the next week could be supported by several observations. Firstly, the conspicuous failure of the company's Keytruda regimen to meet primary goals in a late-stage study for advanced gastric cancer patients may have serious implications on the company's projected earnings and tarnish its innovative reputation. Negative clinical trials can often significantly impact a healthcare company's stock price.\\n\\nFurthermore, the ongoing legal case dealing with Medicare's drug price negotiation plans introduces an element of uncertainty that the stock market generally dislikes. If Merck's lawsuits against the policy fail, it could lead to reduced margins and profit for the company, hurting its financial performance, which is already weak as indicated by negative EBIT per share and EPS.\\n\\nAlthough various news favor the bulls, including the permission to publicly discuss drug price negotiations and the talk around billionaire investors, the weight of the negative news seems overwhelming at this stage, implying potential downside risks. Of course, this does not fully consider potential unexpected news that can always change the stock price direction.\",\n          \"[Positive Developments]:\\n1. Company Strategies & Relationships: The company's strategic moves of forming partnerships and collaborations with big names like Microsoft and Samsung showcases a significant commitment to enhancing product offerings and adaptability to market needs. This may attract investors.\\n\\n2. Company Financials: Their Ev-to-Ebitda ratio (15.8) is fair, indicating that the company is undervalued according to basic investing standards.\\n\\n[Potential Concerns]:\\n1. Frequent Stock Underperformance: In the past few weeks, the stock has underperformed when compared to its competitors in several sessions. This might breed uncertainty among investors.\\n\\n2. Slow Growth Prospects: Some news, such as the headline \\\"Cisco: Sluggish Growth Ahead, But Shares Priced For Pessimism\\\" suggest that future growth may be slow.\\n\\n[Prediction & Analysis]:\\nPrediction: Down by 3-4%\\nAnalysis: Given the combination of negative sentiment from the market and news of sluggish growth predictions, we expect that investor sentiment may slide towards a negative stance in the following week. This sentiment, combined with persistent underperformance in comparison to competitors, could lead to pressure on the stock price. Additionally, while Cisco Systems Inc's strategic partnerships signify enhancements and potential growth in their corporate portfolio, it may not be enough to instigate an immediate bullish trend considering the overall economic climate. This heads up for potential underperformance in the week might discourage trading activity and result in a downward pressure on the stock price. Therefore, despite the strong fundamentals of the company, we predict the stock price to go down by 3-4% in the next week.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"period\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 41,\n        \"samples\": [\n          \"2023-10-29 to 2023-11-05\",\n          \"2023-08-13 to 2023-08-20\",\n          \"2023-07-09 to 2023-07-16\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"down by more than 5%\",\n          \"up by 4-5%\",\n          \"up by 3-4%\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"symbol\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"WMT\",\n          \"MCD\",\n          \"CRM\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3fe8adc",
      "metadata": {
        "id": "a3fe8adc"
      },
      "source": [
        "### 2. Fine-tune LLM\n",
        "\n",
        "This is the core part of fine-tuning, which needs **DeepSpeed** to help you manage your VRAM while training on GPU(s). Since you need a brand new subprocess to launch DeepSpeed, here we don't provide you with the code for fine-tuning. Instead, we highly suggest you to run `train.sh` on your own terminal."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_parquet(\n",
        "    \"hf://datasets/FinGPT/fingpt-forecaster-dow30-202305-202405/data/train-00000-of-00001-7c4c80aa07272d4c.parquet\"\n",
        ")\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "\n",
        "print(train_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3SqU94bq2FR",
        "outputId": "dce92616-afe4-48d8-a8df-34ec57d7dfc8"
      },
      "id": "h3SqU94bq2FR",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': \"[INST]<<SYS>>\\nYou are a seasoned stock market analyst. Your task is to list the positive developments and potential concerns for companies based on relevant news and basic financials from the past weeks, then provide an analysis and prediction for the companies' stock price movement for the upcoming week. Your answer format should be as follows:\\n\\n[Positive Developments]:\\n1. ...\\n\\n[Potential Concerns]:\\n1. ...\\n\\n[Prediction & Analysis]\\nPrediction: ...\\nAnalysis: ...\\n\\n<</SYS>>\\n\\n[Company Introduction]:\\n\\nAmerican Express Co is a leading entity in the Financial Services sector. Incorporated and publicly traded since 1977-05-18, the company has established its reputation as one of the key players in the market. As of today, American Express Co has a market capitalization of 168338.49 in USD, with 723.87 shares outstanding.\\n\\nAmerican Express Co operates primarily in the US, trading under the ticker AXP on the NEW YORK STOCK EXCHANGE, INC.. As a dominant force in the Financial Services space, the company continues to innovate and drive progress within the industry.\\n\\nFrom 2023-05-07 to 2023-05-14, AXP's stock price decreased from 150.55 to 145.90. News during this period are listed below:\\n\\n[Headline]: Credit cards: issuers could use their own flexible friends\\n[Summary]: Concerns profitability has peaked following a strong 2022 are not baseless, but even so credit cards remain a lucrative business\\n\\n[Headline]: Dow's nearly 175-point fall led by losses in American Express, Nike shares\\n[Summary]: Dragged down by losses for shares of American Express and Nike, the Dow Jones Industrial Average is trading down Wednesday afternoon. Shares of American...\\n\\n[Headline]: Chase is opening its first U.S. airport lounge in Boston  here's how you can get in\\n[Summary]: The new Chase Sapphire Lounge at Boston Airport features complimentary food, drinks, rest pods, massage chairs and more. Here's how you can get in.\\n\\n[Headline]: American Express Co. stock outperforms competitors despite losses on the day\\n[Summary]: Shares of American Express Co. sank 0.04% to $147.93 Friday, on what proved to be an all-around dismal trading session for the stock market, with the S&P 500...\\n\\n[Headline]: How To Build A Dividend Portfolio With $25,000 Among May's Top 30 Stocks\\n[Summary]: An investment strategy that combines dividend income with dividend growth, brings several benefits for dividend income investors. Click here to read more.\\n\\nSome recent basic financials of AXP, reported at 2023-03-31, are presented below:\\n\\n[Basic Financials]:\\n\\nassetTurnoverTTM: 0.2614\\nbookValue: 25992\\ncashRatio: 0.2944926548987087\\ncurrentRatio: 0.7185\\nebitPerShare: 2.9126\\neps: 2.4409\\nev: 246452.85\\nfcfPerShareTTM: 20.2611\\ngrossMargin: 0.624\\nlongtermDebtTotalAsset: 0.1744\\nlongtermDebtTotalCapital: 0.2169\\nlongtermDebtTotalEquity: 1.5827\\nnetDebtToTotalCapital: 0.6532\\nnetDebtToTotalEquity: 4.7667\\nnetMargin: 0.1189\\noperatingMargin: 0.1419\\npayoutRatioTTM: 0.2247\\npb: 4.7152\\npeTTM: 16.9489\\npretaxMargin: 0.1419\\npsTTM: 2.1201\\nquickRatio: 0.7185\\nreceivablesTurnoverTTM: 1.0476\\nroaTTM: 0.0327\\nroeTTM: 0.2955\\nroicTTM: 0.0412\\nrotcTTM: 0.0515\\nsalesPerShare: 20.5309\\nsgaToSale: 0.376\\ntotalDebtToEquity: 6.2969\\ntotalDebtToTotalAsset: 0.694\\ntotalDebtToTotalCapital: 0.863\\ntotalRatio: 1.1239\\n\\nBased on all the information before 2023-05-14, let's first analyze the positive developments and potential concerns for AXP. Come up with 2-4 most important factors respectively and keep them concise. Most factors should be inferred from company related news. Then make your prediction of the AXP cryptocurrency price movement for next week (2023-05-14 to 2023-05-21). Provide a summary analysis to support your prediction.[/INST]\", 'answer': \"[Positive Developments]:\\n1. Market Outperformance: Despite the recent fall, American Express' stock has outperformed competitors, implying strong fundamentals and investor confidence.\\n2. Strong Margins: The company has a strong gross margin of 62.4% and an even stronger operating margin of 14.19%, indicating good past profitability.\\n3. Solid Return on Equity (ROE): AXPs ROE of 0.2955 suggests that it is effective at generating return on equity.\\n\\n[Potential Concerns]:\\n1. High Debt Ratio: AXP's high total debt to equity ratio of 6.2969 means it relies heavily on debt to finance its assets which could be concerning in an adverse economic climate.\\n2. Decreasing Stock Price: Over the past week, AXP's stock price has fallen from 150.55 to 145.90. This suggests a downward trend may be in progress.\\n3. Market Losses: The recent losses in American Express and other Dow Jones stocks indicate a challenging market environment.\\n\\n[Prediction & Analysis]:\\nPrediction: Up by 3-4%\\nAnalysis: In spite of the potential concerns, the strong fundamentals of American Express offer promising prospects. Their strong gross and operating margins suggest that they have an effective cost strategy which motivates potential future investments and revenue generation. \\n\\nIn addition, their ROE suggests they are capitalizing effectively on their equity. While the high debt ratio and falling stock price are potential concerns, these may offer buying opportunities for investors as they expect a rebound. \\n\\nMoreover, the stock has shown strength against other stocks, indicating that it remains a favored investment. Considering these factors, it is likely that the stock could see an uptrend in the coming week. A possible increase in the stock price of approximately 3-4% is anticipated. This outlook assumes stability in the overall market conditions and no significant negative financial news for the firm. \\n\\nHowever, investors should closely watch the company's debt levels and the broader market trends to ensure these do not adversely impact the stock's performance.\", 'period': '2023-05-14 to 2023-05-21', 'label': 'up by 3-4%', 'symbol': 'AXP'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "cache_dir = \"./hf_cache\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    cache_dir=cache_dir,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Define BitsAndBytesConfig for 8-bit quantization\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config, # Use quantization_config instead of load_in_8bit\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    cache_dir=cache_dir,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "945551b24937492a87234945219b781f",
            "cdde208b6dc4401897f2b8a97bdb070b",
            "dfe0079b08e54b79b29c641a7c14c02a",
            "faf054d3370842bca00b268629876d76",
            "6263855ed9204b9996ae3e9a6b597d36",
            "d43e02699690474b817578d9484f8286",
            "3b0944fbe8b044c6b7c678524b348ce9",
            "194259d221ac426fb1057427c3e28dc7",
            "a5041052c9ff4fdab4972e4b5dcffec9",
            "c337029540a24a6e94077d971ab48bd6",
            "3e3da0c3a99041a7ba9522f683d858ae"
          ]
        },
        "id": "Ryyn7I3Vp9b4",
        "outputId": "cd7be574-e81d-4070-f4a9-367ff74270a4"
      },
      "id": "Ryyn7I3Vp9b4",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/338 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "945551b24937492a87234945219b781f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_fn(batch):\n",
        "    texts = [\n",
        "        p + tokenizer.eos_token + a\n",
        "        for p, a in zip(batch[\"prompt\"], batch[\"answer\"])\n",
        "    ]\n",
        "\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=1024,\n",
        "        padding=False,\n",
        "    )"
      ],
      "metadata": {
        "id": "PtYjBTb9uCfU"
      },
      "id": "PtYjBTb9uCfU",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ],
      "metadata": {
        "id": "UfISbw_euo1c"
      },
      "id": "UfISbw_euo1c",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(\n",
        "    tokenize_fn,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        ")\n",
        "\n",
        "test_dataset = test_dataset.map(\n",
        "    tokenize_fn,\n",
        "    batched=True,\n",
        "    remove_columns=test_dataset.column_names,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "5075db52b2bf4673a02049cdb1393fcd",
            "53ef3d8207f543e88b60a81dd137ba88",
            "935e186acfa44846b3cff1a4c44190de",
            "79f9d365186640ea97629723d3dfb3df",
            "5a9c85696e8e468c977f73b7f76c5333",
            "6394a5e3ca4c4f05ae9ce6a5f572cdb4",
            "e5b85dfef0f442cda33b46ee54b5f25f",
            "af7a78d622a84c71a1e45f4162de2f97",
            "eb7149b46ac44229aae84fdacb2c2ef7",
            "f658d94f7242454a89348b4a53d9cdde",
            "26669556bfa14b1aa0f4b841d5a85fbb",
            "176d92a9ad5041649e80558448f3efa5",
            "d8415d69f8ae4d569f8d31a5146cc3a7",
            "bdaa16b5e0524f84854c16715ce51069",
            "607314b73a024c32b20a50eb475e65b6",
            "1240e3acdae347ce98cadef801a9372f",
            "1b0efea033a64ccba984155f47d12433",
            "c99d7f1b5e7341beab7be05ae1bb4b2d",
            "604aeadb025d41239087d900967e01c5",
            "677b93894127442faf382af3bf9f2602",
            "56638d9900714f599d5ed1e0a1f816a7",
            "290ccb34c57f42a69c9d4c3735532430"
          ]
        },
        "id": "1vY8phk9ursP",
        "outputId": "0b70927f-6ff2-4bbf-dce1-e86403bfbae9"
      },
      "id": "1vY8phk9ursP",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1230 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5075db52b2bf4673a02049cdb1393fcd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "176d92a9ad5041649e80558448f3efa5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8pfy5hguzNA",
        "outputId": "1fdcdfbc-662d-4ce2-d6ed-ec0035513573"
      },
      "id": "l8pfy5hguzNA",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 1230\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.select(range(50))\n",
        "test_dataset = test_dataset.select(range(20))"
      ],
      "metadata": {
        "id": "UMEn-xV1mll9"
      },
      "id": "UMEn-xV1mll9",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lLTkur0vxGr",
        "outputId": "dbf75045-a74c-494e-e92c-3ff9b5f80077"
      },
      "id": "_lLTkur0vxGr",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 9,232,384 || all params: 1,552,946,688 || trainable%: 0.5945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")"
      ],
      "metadata": {
        "id": "8Eo10Oy2vzvb"
      },
      "id": "8Eo10Oy2vzvb",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora_out\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    max_steps=5,\n",
        "    logging_steps=1,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=1,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    report_to=\"none\",\n",
        "    dataloader_pin_memory=False,\n",
        "    remove_unused_columns=False,\n",
        ")"
      ],
      "metadata": {
        "id": "xVrRzhB0v8tu"
      },
      "id": "xVrRzhB0v8tu",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "6n-9VMHKiEZv"
      },
      "id": "6n-9VMHKiEZv",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from transformers import Trainer\n",
        "\n",
        "#trainer = Trainer(\n",
        "    #model=model,\n",
        "    #args=training_args,\n",
        "    #train_dataset=train_dataset,\n",
        "    #data_collator=data_collator,\n",
        "#)"
      ],
      "metadata": {
        "id": "OnPcpg0xv_5A"
      },
      "id": "OnPcpg0xv_5A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trainer.train()"
      ],
      "metadata": {
        "id": "3rDZzR4uwQm3"
      },
      "id": "3rDZzR4uwQm3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "sample = train_dataset[0]\n",
        "\n",
        "batch = {\n",
        "    \"input_ids\": torch.tensor(sample[\"input_ids\"]).unsqueeze(0),\n",
        "    \"attention_mask\": torch.tensor(sample[\"attention_mask\"]).unsqueeze(0),\n",
        "}\n",
        "\n",
        "batch = {k: v.to(model.device) for k, v in batch.items()}"
      ],
      "metadata": {
        "id": "ucuCISjxrTdK"
      },
      "id": "ucuCISjxrTdK",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "outputs = model(**batch, labels=batch[\"input_ids\"])\n",
        "loss = outputs.loss\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print(\"Training loss:\", loss.item())"
      ],
      "metadata": {
        "id": "CEkOwZB7rV8x"
      },
      "id": "CEkOwZB7rV8x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4b9e630f",
      "metadata": {
        "id": "4b9e630f"
      },
      "source": [
        "### 3. Have a try on your own fine-tuned LLMs\n",
        "\n",
        "In this part, you can have a try on your fine-tuned models by providing it with some inputs and see their responses.\n",
        "\n",
        "If you made it, congratulations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f9bcc86",
      "metadata": {
        "id": "9f9bcc86"
      },
      "outputs": [],
      "source": [
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#cache_dir = \"./pretrained-models\" ### Adjust based on your setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac2c8808",
      "metadata": {
        "id": "ac2c8808"
      },
      "outputs": [],
      "source": [
        "#base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    #\"your_base_model\", ### Change to your base model\n",
        "    #trust_remote_code=True,\n",
        "    #device_map=\"auto\",\n",
        "    #cache_dir=cache_dir,\n",
        "    #torch_dtype=torch.float16,   # optional if you have enough VRAM\n",
        "#)\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\n",
        "    #'your_base_model', ### Change to your base model\n",
        "    #cache_dir=cache_dir,\n",
        "#)\n",
        "\n",
        "#model = PeftModel.from_pretrained(\n",
        "    #base_model,\n",
        "    #'your_finetuned_model', ### Change to your fine-tuned model\n",
        "    #cache_dir=cache_dir,\n",
        "    # offload_folder=\"./offload2/\",\n",
        "    #torch_dtype=torch.float16,\n",
        "    # offload_buffers=True\n",
        "#)\n",
        "#model = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "295a72fa",
      "metadata": {
        "id": "295a72fa"
      },
      "outputs": [],
      "source": [
        "#prompt = \"\"\"\n",
        "    #Your prompt here\n",
        "#\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94b4a607",
      "metadata": {
        "id": "94b4a607"
      },
      "outputs": [],
      "source": [
        "#inputs = tokenizer(\n",
        "    #prompt,\n",
        "    #return_tensors='pt',\n",
        "    #max_length=4096,\n",
        "    #padding=False,\n",
        "    #truncation=True\n",
        "#)\n",
        "#inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
        "\n",
        "#res = model.generate(\n",
        "    #**inputs, max_length=4096, do_sample=True,\n",
        "    #eos_token_id=tokenizer.eos_token_id,\n",
        "    #use_cache=True\n",
        "#)\n",
        "#output = tokenizer.decode(res[0], skip_special_tokens=True)\n",
        "#answer = re.sub(r'.*\\[/INST\\]\\s*', '', output, flags=re.DOTALL) # don't forget to import re\n",
        "#print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Financial Data in the form of Text\n",
        "financial_context = \"\"\"\n",
        "Dow Jones Index fell by 1.2% today.\n",
        "Major declines were observed in financial and technology stocks.\n",
        "Investors showed concerns about interest rate uncertainty and inflation.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "You are a financial analyst.\n",
        "\n",
        "Market information:\n",
        "{financial_context}\n",
        "\n",
        "Question:\n",
        "What is today's market sentiment and possible short-term outlook?\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=120\n",
        "    )\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775,
          "referenced_widgets": [
            "1fb2a20baf67417fb020a91c07da2baa",
            "563e65a5b02b43b28ea8523e66508fe9",
            "54ce3b33427944ca964de734e723df3e",
            "33bf9e3819314e989b4d3c4672141fc0",
            "0d186a32574d4ed3ad6d48ee53fe9fc5",
            "e6ae4e71f30044108d5f302b4c56af12",
            "01ba9bbb4165407d8a08dcec22f33aaa",
            "dba540cb79ff4a2694c10354d4b9e1f9",
            "83f3fba03d004191a888f67f6715d51e",
            "99ea5da0f5e844adaf7804804634f04c",
            "8a5c1e570b69433c896053292b7916f8",
            "38ff0c3bf3a44188a2c29d6c89e5180c",
            "e87d8c48c6c943a6801de008e08dc708",
            "18aa7656aa7a4fef91f194d21a3eff5e",
            "7c72deb61a0e4a7393370ac92bf800b2",
            "0709a9c6b75b4473b7786af987c66e00",
            "619df901f8e2438eaa35088d66c72f08",
            "60ac0f02c8514235bdada4a2d50d72e2",
            "8a642b48bf514e70902edd329006cbd8",
            "f6f4ecc66f254bde9c4a408dfabad45b",
            "e8640ed9938b4b9aab0417432d8251ec",
            "2cab26bd1f0c49c89d7bb3f510c63094",
            "100b377bfadc41c091aa9a59909e83bf",
            "006d24ce7f454488b991d0cac736b41b",
            "9e9211e9d3874a6ea224237f3ff8cffa",
            "c02174c60c1c4543a510fc98222e2738",
            "2936c3166d1f4830a605e3ecb7f3dd0e",
            "aa8738270d2a4b8ab2c9b609efb98271",
            "0f9524a9971b4ebfb482bb8be0b17b6c",
            "8b8a9a6d1e594d9ead81ad932bfde840",
            "629a9593d5094286b4fdf6b349364981",
            "6cb199d6cb5d4d7688be30557a0bd53b",
            "986decac36664cf79707d8924e7847be",
            "2c7dc2289e8840a08420458affc1b4de",
            "a58abffd94f74bcc9d9e23d453b52f72",
            "916e42fccb764de19af90139b5759ee0",
            "380b655f0f4542678c80c8d25cefc996",
            "436814b307b74f29ad441050dbc8b4f4",
            "42c5579926844c8db9a33d3d5383179b",
            "fe6bced3789c413abaee8b242057376b",
            "43db279c3970437191648243b00a169d",
            "f045f9ff5ccf481c9e168552446bb628",
            "cf3fd0aa1c9f4df7adf38137b6ca830a",
            "304558ed5d73458091b9c3444517d589",
            "cf70ee516f3a46f992b75ab15b7b1927",
            "c57383da10184ffca1459f4cce846da3",
            "c5fe9d16c36b46c28ea4076eb33c2fd3",
            "52d6079c21114f4fb55cc70c6cb71747",
            "5fb67329132642c5b97c7507b9c8d53c",
            "5fedc0ed5ba045dca897f0eee3d4dd78",
            "11c9007a17a34592b2925b18bc12091f",
            "0f0869776aa04a97b03d071ebaaa95ff",
            "018b1fd05bb94129b0e2083c79e3dafe",
            "75ce3e1fe2784368906df645abffe4e2",
            "5ebfa2e7891d44559338ad4b8d463217",
            "7b01424acf32473ea090fd0f1b94a3ba",
            "26a0a247e69b434cb9f0a60c9ea83028",
            "4e422ba8e37c442cb12ce1b514a7dcc3",
            "aaef335d9b6741c4a60e414f256b22b1",
            "ec2804e72d8841b9abc89e7946d8a270",
            "373ce7540bb649409b66ddad50799dbd",
            "ef8fbac29b0c4e44b9ba5a2eb8bcdfc5",
            "685dcf6742054c6199c773dae09e44a0",
            "5e53b0f5f1ad4e62a32306dcf7765e13",
            "2fb2dddce2d144b9a37d3913bcf93b0d",
            "0fcb1cd320b8499b8d2f6cdf530e9b23",
            "382e09d1e0214b95b17457f92a6ea556",
            "92f60d48531d48b1b140479e6bca6bf0",
            "d246aae51b1145fcbf3fc4c8c0c06a0a",
            "d0a8d3170bdd467d92be30b835a8ca0e",
            "596bff4aaf7d42d5b8b511f0ce58ca99",
            "2c12b5580b054cc2ad3df71691c8e907",
            "be6b849b38fb406380fd689d4f2f17a8",
            "73bcb13c452c4cf2a40def3901db2b21",
            "b147ca6d80254007a7ddb27e8716efe2",
            "5ec600b69dcd4cce900141396f163f1a",
            "366f8799850c4e6d89e4e064142f0b02",
            "a1ddddc1f82d4d2a831c4076fd5ca9b4",
            "3bec1f6d4c8c437690bd862d16374e7f",
            "6db88547d3514e04897087990aa7c03b",
            "240b4e03c33f42aaa1e470c15c1c7816",
            "6fa5121bba6841cc9726ba5682124a6f",
            "c8b9d29b17a044baa25cd28c3d549cce",
            "c0cd319c63d14468b16b47b78ac2696f",
            "79889db8a05f4dcf9d71d211bb88b579",
            "ebffd6121bff49299809ff373cc95036",
            "a87b78b67f9b4c34a4b02c86fea1eedc",
            "8cb732a0483445d8bbfc66bb2f36b1bf"
          ]
        },
        "id": "HgMqktZ1vpFk",
        "outputId": "c4d8c052-c61b-41e8-e173-db049d594871"
      },
      "id": "HgMqktZ1vpFk",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1fb2a20baf67417fb020a91c07da2baa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38ff0c3bf3a44188a2c29d6c89e5180c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "100b377bfadc41c091aa9a59909e83bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c7dc2289e8840a08420458affc1b4de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf70ee516f3a46f992b75ab15b7b1927"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b01424acf32473ea090fd0f1b94a3ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/338 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "382e09d1e0214b95b17457f92a6ea556"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1ddddc1f82d4d2a831c4076fd5ca9b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a financial analyst.\n",
            "\n",
            "Market information:\n",
            "\n",
            "Dow Jones Index fell by 1.2% today.\n",
            "Major declines were observed in financial and technology stocks.\n",
            "Investors showed concerns about interest rate uncertainty and inflation.\n",
            "\n",
            "\n",
            "Question:\n",
            "What is today's market sentiment and possible short-term outlook?\n",
            "Today's market sentiment seems to be pessimistic, with investors expressing concern over the uncertain nature of interest rates and potential inflationary pressures. The Dow Jones index fell by 1.2%, highlighting that major sectors including financials and technology have been particularly affected negatively. Investors may expect volatility ahead as they navigate through these uncertainties. \n",
            "\n",
            "Possible short-term outlook: \n",
            "- There might be further decline in stock prices due to continued uncertainty around monetary policy and economic growth expectations.\n",
            "- Market participants could seek out defensive assets like bonds or cash equivalents to protect against further losses.\n",
            "- Some traders may take advantage of the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b80e700",
      "metadata": {
        "id": "5b80e700"
      },
      "source": [
        "### 4. Comparison\n",
        "\n",
        "Now since you get both fine-tuned LLMs based on DeepSeek, Llama3 and Qwen, here we would like you to do some comparison on selected metrics, to see which of these 2 fine-tuned models performs best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "670693a3",
      "metadata": {
        "id": "670693a3"
      },
      "outputs": [],
      "source": [
        "#import os\n",
        "#import torch\n",
        "#import re\n",
        "#from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "#from peft import PeftModel\n",
        "#from datasets import load_from_disk\n",
        "#from datasets import Dataset\n",
        "#from sklearn.metrics import accuracy_score\n",
        "#from tqdm import tqdm\n",
        "## from peft import PeftModel\n",
        "#from utils import *\n",
        "#import time\n",
        "#import json, pickle\n",
        "\n",
        "#os.environ[\"HUGGINGFACE_TOKEN\"] = \"your huggingface token\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec127051",
      "metadata": {
        "id": "ec127051"
      },
      "outputs": [],
      "source": [
        "# Llama3\n",
        "#llama3_base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    #'meta-llama/Llama-3.1-8B',\n",
        "    #trust_remote_code=True,\n",
        "    #device_map=\"auto\",\n",
        "    #cache_dir=cache_dir,\n",
        "    #torch_dtype=torch.float16,\n",
        "#)\n",
        "\n",
        "#llama3_model = PeftModel.from_pretrained(\n",
        "    #llama3_base_model,\n",
        "    #'your_finetuned_model', ### Change to your fine-tuned model\n",
        "    #cache_dir=cache_dir,\n",
        "    #torch_dtype=torch.float16,\n",
        "#)\n",
        "#llama3_model = llama3_model.eval()\n",
        "\n",
        "#llama3_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    #'meta-llama/Llama-3.1-8B',\n",
        "    #cache_dir=cache_dir,\n",
        "#)\n",
        "#llama3_tokenizer.padding_side = \"right\"\n",
        "#llama3_tokenizer.pad_token_id = llama3_tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "cache_dir = \"./hf_cache\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "83nXB5-_ApGw"
      },
      "id": "83nXB5-_ApGw",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e8ae5b26",
      "metadata": {
        "id": "e8ae5b26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ed602913e67b4753a9bb8c83ff97bc46",
            "1caeeb98ce8a4323bbd1a75bc87a70a5",
            "55a972c5f02c492c99358a9611627285",
            "92f79931bbf94c14b90bf84a67eb5a91",
            "db5d8b42fa2d4e8493b6029dce22f2fa",
            "f3ea88ed30e8425f8bfc2d27795d4fd7",
            "991d612451134bcd859482fd82ef35e8",
            "9d6d6866a3a743ccb71372c1b9070e54",
            "b06f5a3636fd48d1af8917aef6dbf36c",
            "b827c69787d4485d818f94f581e59549",
            "06a0c9095a2341718a69fe72a74c2cdc"
          ]
        },
        "outputId": "4e2a1736-dcbf-45c3-829d-b2594de5d3b1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed602913e67b4753a9bb8c83ff97bc46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.0.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.0.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.0.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.0.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.0.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.0.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.0.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.0.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.0.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.1.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.1.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.1.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.1.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.1.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.1.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.1.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.1.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.1.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.2.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.2.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.2.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.2.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.2.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.2.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.2.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.2.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.2.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.3.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.3.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.3.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.3.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.3.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.3.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.3.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.3.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.3.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.4.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.4.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.4.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.4.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.4.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.4.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.4.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.4.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.4.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.5.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.5.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.5.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.5.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.5.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.5.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.5.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.5.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.5.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.6.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.6.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.6.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.6.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.6.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.6.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.6.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.6.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.6.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.7.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.7.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.7.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.7.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.7.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.7.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.7.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.7.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.7.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.8.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.8.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.8.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.8.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.8.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.8.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.8.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.8.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.8.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.9.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.9.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.9.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.9.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.9.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.9.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.9.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.9.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.9.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.10.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.10.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.10.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.10.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.10.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.10.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.10.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.10.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.10.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.11.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.11.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.11.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.11.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.11.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.11.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.11.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.11.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.11.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.12.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.12.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.12.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.12.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.12.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.12.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.12.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.12.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.12.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.13.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.13.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.13.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.13.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.13.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.13.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.13.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.13.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.13.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.14.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.14.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.14.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.14.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.14.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.14.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.14.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.14.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.14.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.15.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.15.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.15.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.15.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.15.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.15.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.15.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.15.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.15.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.16.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.16.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.16.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.16.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.16.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.16.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.16.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.16.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.16.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.17.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.17.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.17.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.17.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.17.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.17.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.17.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.17.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.17.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.18.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.18.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.18.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.18.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.18.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.18.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.18.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.18.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.18.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.19.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.19.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.19.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.19.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.19.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.19.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.19.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.19.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.19.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.20.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.20.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.20.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.20.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.20.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.20.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.20.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.20.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.20.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.21.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.21.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.21.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.21.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.21.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.21.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.21.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.21.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.21.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.22.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.22.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.22.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.22.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.22.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.22.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.22.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.22.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.22.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.23.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.23.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.23.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.23.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.23.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.23.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.23.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.23.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.23.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.24.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.24.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.24.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.24.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.24.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.24.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.24.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.24.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.24.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.25.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.25.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.25.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.25.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.25.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.25.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.25.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.25.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.25.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.26.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.26.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.26.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.26.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.26.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.26.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.26.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.26.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.26.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.27.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.27.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.27.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.27.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.27.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.27.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.27.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.27.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.27.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.28.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.28.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.28.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.28.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.28.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.28.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.28.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.28.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.28.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.29.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.29.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.29.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.29.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.29.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.29.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.29.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.29.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.29.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.30.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.30.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.30.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.30.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.30.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.30.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.30.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.30.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.30.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.31.self_attn.q_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.31.self_attn.k_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.31.self_attn.v_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.31.self_attn.o_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.31.mlp.gate_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.31.mlp.up_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.31.mlp.down_proj.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.31.input_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.layers.31.post_attention_layernorm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to model.norm.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Can't find 'adapter_config.json' at 'your_finetuned_model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '401 Unauthorized' for url 'https://huggingface.co/your_finetuned_model/resolve/main/adapter_config.json'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 config_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    318\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, etag_timeout, token, local_files_only, headers, endpoint, tqdm_class, dry_run)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1033\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, token, local_files_only, force_download, tqdm_class, dry_run)\u001b[0m\n\u001b[1;32m   1182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhead_call_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m             \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1811\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1812\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1813\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder, retry_on_errors)\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1699\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1700\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, timeout, library_name, library_version, user_agent, headers, endpoint, retry_on_errors)\u001b[0m\n\u001b[1;32m   1621\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1622\u001b[0;31m     response = _httpx_follow_relative_redirects(\n\u001b[0m\u001b[1;32m   1623\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_headers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_on_errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_httpx_follow_relative_redirects\u001b[0;34m(method, url, retry_on_errors, **httpx_kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m         )\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    709\u001b[0m             )\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-6998e98f-27508b5e7b8abf812fd07d16;8da4f4a3-2eca-436c-91f4-0fb54201da2e)\n\nRepository Not Found for url: https://huggingface.co/your_finetuned_model/resolve/main/adapter_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nInvalid username or password.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1421769802.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m deepseek_model = PeftModel.from_pretrained(\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mdeepseek_base_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;34m'your_finetuned_model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m### Change to your fine-tuned model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_auth_token\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_auth_token\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                 \u001b[0mhf_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_auth_token\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             config = PEFT_TYPE_TO_CONFIG_MAPPING[PeftConfig._get_peft_type(model_id, **hf_kwargs)].from_pretrained(\n\u001b[0m\u001b[1;32m    460\u001b[0m                 \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 )\n\u001b[1;32m    322\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find '{CONFIG_NAME}' at '{model_id}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mloaded_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at 'your_finetuned_model'"
          ]
        }
      ],
      "source": [
        "# DeepSeek\n",
        "#deepseek_base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    #'deepseek-ai/DeepSeek-R1-Distill-Llama-8B',\n",
        "    #trust_remote_code=True,\n",
        "    #device_map=\"auto\",\n",
        "    #cache_dir=cache_dir,\n",
        "    #torch_dtype=torch.float16,\n",
        "#)\n",
        "\n",
        "#deepseek_model = PeftModel.from_pretrained(\n",
        "    #deepseek_base_model,\n",
        "    #'your_finetuned_model', ### Change to your fine-tuned model\n",
        "    #cache_dir=cache_dir,\n",
        "    #torch_dtype=torch.float16,\n",
        "#)\n",
        "##deepseek_model = deepseek_model.eval()\n",
        "\n",
        "deepseek_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    #'deepseek-ai/DeepSeek-R1-Distill-Llama-8B',\n",
        "    #cache_dir=cache_dir,\n",
        ")\n",
        "#deepseek_tokenizer.padding_side = \"right\"\n",
        "#deepseek_tokenizer.pad_token_id = deepseek_tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30501d01",
      "metadata": {
        "id": "30501d01"
      },
      "outputs": [],
      "source": [
        "#test_dataset = load_dataset(\"your_dataset_name\")[0][\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26e5f36d",
      "metadata": {
        "id": "26e5f36d"
      },
      "outputs": [],
      "source": [
        "#def filter_by_ticker(test_dataset, ticker_code):\n",
        "\n",
        "    #filtered_data = []\n",
        "\n",
        "    #for row in test_dataset:\n",
        "        #prompt_content = row['prompt']\n",
        "\n",
        "        #ticker_symbol = re.search(r\"ticker\\s([A-Z]+)\", prompt_content)\n",
        "\n",
        "        #if ticker_symbol and ticker_symbol.group(1) == ticker_code:\n",
        "            #filtered_data.append(row)\n",
        "\n",
        "    #filtered_dataset = Dataset.from_dict({key: [row[key] for row in filtered_data] for key in test_dataset.column_names})\n",
        "\n",
        "    #return filtered_dataset\n",
        "\n",
        "#def get_unique_ticker_symbols(test_dataset):\n",
        "\n",
        "    #ticker_symbols = set()\n",
        "\n",
        "    #for i in range(len(test_dataset)):\n",
        "        #prompt_content = test_dataset[i]['prompt']\n",
        "\n",
        "        #ticker_symbol = re.search(r\"ticker\\s([A-Z]+)\", prompt_content)\n",
        "\n",
        "        #if ticker_symbol:\n",
        "            #ticker_symbols.add(ticker_symbol.group(1))\n",
        "\n",
        "    #return list(ticker_symbols)\n",
        "\n",
        "#def insert_guidance_after_intro(prompt):\n",
        "\n",
        "    #intro_marker = (\n",
        "        #\"[INST]<<SYS>>\\n\"\n",
        "        #\"You are a seasoned stock market analyst. Your task is to list the positive developments and \"\n",
        "        #\"potential concerns for companies based on relevant news and basic financials from the past weeks, \"\n",
        "        #\"then provide an analysis and prediction for the companies' stock price movement for the upcoming week.\"\n",
        "    #guidance_start_marker = \"Based on all the information before\"\n",
        "    #guidance_end_marker = \"Following these instructions, please come up with 2-4 most important positive factors\"\n",
        "\n",
        "    #intro_pos = prompt.find(intro_marker)\n",
        "    #guidance_start_pos = prompt.find(guidance_start_marker)\n",
        "    #guidance_end_pos = prompt.find(guidance_end_marker)\n",
        "\n",
        "    #if intro_pos == -1 or guidance_start_pos == -1 or guidance_end_pos == -1:\n",
        "        #return prompt\n",
        "\n",
        "    #guidance_section = prompt[guidance_start_pos:guidance_end_pos].strip()\n",
        "\n",
        "    #new_prompt = (\n",
        "        #f\"{prompt[:intro_pos + len(intro_marker)]}\\n\\n\"\n",
        "        #f\"{guidance_section}\\n\\n\"\n",
        "        #f\"{prompt[intro_pos + len(intro_marker):guidance_start_pos]}\"\n",
        "        #f\"{prompt[guidance_end_pos:]}\"\n",
        "    #)\n",
        "\n",
        "    #return new_prompt\n",
        "\n",
        "\n",
        "#def apply_to_all_prompts_in_dataset(test_dataset):\n",
        "\n",
        "    #updated_dataset = test_dataset.map(lambda x: {\"prompt\": insert_guidance_after_intro(x[\"prompt\"])})\n",
        "\n",
        "    #return updated_dataset\n",
        "\n",
        "#test_dataset = apply_to_all_prompts_in_dataset(test_dataset)\n",
        "\n",
        "#unique_symbols = set(test_dataset['symbol'])\n",
        "\n",
        "#def test_demo(model, tokenizer, prompt):\n",
        "\n",
        "    #inputs = tokenizer(\n",
        "        #prompt, return_tensors='pt',\n",
        "        #padding=False, max_length=8000\n",
        "    #)\n",
        "    # inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
        "\n",
        "    # start_time = time.time()\n",
        "    # res = model.generate(\n",
        "    #     **inputs, max_length=4096, do_sample=True,\n",
        "    #     eos_token_id=tokenizer.eos_token_id,\n",
        "    #     use_cache=True\n",
        "    # )\n",
        "#     end_time = time.time()\n",
        "#     output = tokenizer.decode(res[0], skip_special_tokens=True)\n",
        "#     return output, end_time - start_time\n",
        "\n",
        "# def test_acc(test_dataset, modelname):\n",
        "#     answers_base, answers_fine_tuned, gts, times_base, times_fine_tuned = [], [], [], [], []\n",
        "#     if modelname == \"llama3\":\n",
        "#         base_model = llama3_base_model\n",
        "#         model = llama3_model\n",
        "#         tokenizer = llama3_tokenizer\n",
        "#     elif modelname == \"deepseek\":\n",
        "#         base_model = deepseek_base_model\n",
        "#         model = deepseek_model\n",
        "#         tokenizer = deepseek_tokenizer\n",
        "#     ### Add other models here\n",
        "#     elif modelname == \"your_model_name\":  # Add other models as needed\n",
        "#         base_model = your_base_model  # Define your base model\n",
        "#         model = your_finetuned_model  # Define your fine-tuned model\n",
        "#         tokenizer = your_tokenizer     # Define your tokenizer\n",
        "\n",
        "#     for i in tqdm(range(len(test_dataset)), desc=\"Processing test samples\"):\n",
        "#         try:\n",
        "#             prompt = test_dataset[i]['prompt']\n",
        "#             gt = test_dataset[i]['answer']\n",
        "\n",
        "#             output_base, time_base = test_demo(base_model, tokenizer, prompt)\n",
        "#             answer_base = re.sub(r'.*\\[/INST\\]\\s*', '', output_base, flags=re.DOTALL)\n",
        "\n",
        "#             output_fine_tuned, time_fine_tuned = test_demo(model, tokenizer, prompt)\n",
        "#             answer_fine_tuned = re.sub(r'.*\\[/INST\\]\\s*', '', output_fine_tuned, flags=re.DOTALL)\n",
        "\n",
        "#             answers_base.append(answer_base)\n",
        "#             answers_fine_tuned.append(answer_fine_tuned)\n",
        "#             gts.append(gt)\n",
        "#             times_base.append(time_base)\n",
        "#             times_fine_tuned.append(time_fine_tuned)\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error processing sample {i}: {e}\")\n",
        "#     return answers_base, answers_fine_tuned, gts, times_base, times_fine_tuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87608f38",
      "metadata": {
        "id": "87608f38"
      },
      "outputs": [],
      "source": [
        "### Llama3 Result Evaluating\n",
        "\n",
        "# llama3_answers_base, llama3_answers_fine_tuned, llama3_gts, llama3_base_times, llama3_fine_tuned_times = test_acc(test_dataset, \"llama3\")\n",
        "# llama3_base_metrics = calc_metrics(llama3_answers_base, llama3_gts)\n",
        "# llama3_fine_tuned_metrics = calc_metrics(llama3_answers_fine_tuned, llama3_gts)\n",
        "\n",
        "# with open(\"./comparison_results/llama3_base_metrics.pkl\", \"wb\") as f:\n",
        "#     pickle.dump(llama3_base_metrics, f)\n",
        "\n",
        "# with open(\"./comparison_results/llama3_fine_tuned_metrics.pkl\", \"wb\") as f:\n",
        "#     pickle.dump(llama3_fine_tuned_metrics, f)\n",
        "\n",
        "# with open(\"./comparison_results/llama3_base_times.pkl\", \"wb\") as f:\n",
        "#     pickle.dump(llama3_base_times, f)\n",
        "\n",
        "# with open(\"./comparison_results/llama3_fine_tuned_times.pkl\", \"wb\") as f:\n",
        "#     pickle.dump(llama3_fine_tuned_times, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82b5df00",
      "metadata": {
        "id": "82b5df00"
      },
      "outputs": [],
      "source": [
        "### DeepSeek Result Evaluating\n",
        "\n",
        "# deepseek_answers_base, deepseek_answers_fine_tuned, deepseek_gts, deepseek_base_times, deepseek_fine_tuned_times = test_acc(test_dataset, \"deepseek\")\n",
        "# deepseek_base_metrics = calc_metrics(deepseek_answers_base, deepseek_gts)\n",
        "# deepseek_fine_tuned_metrics = calc_metrics(deepseek_answers_fine_tuned, deepseek_gts)\n",
        "\n",
        "# with open(\"./comparison_results/deepseek_base_metrics.pkl\", \"wb\") as f:\n",
        "#     pickle.dump(deepseek_base_metrics, f)\n",
        "\n",
        "# with open(\"./comparison_results/deepseek_fine_tuned_metrics.pkl\", \"wb\") as f:\n",
        "#     pickle.dump(deepseek_fine_tuned_metrics, f)\n",
        "\n",
        "# with open(\"./comparison_results/deepseek_base_times.pkl\", \"wb\") as f:\n",
        "#     pickle.dump(deepseek_base_times, f)\n",
        "\n",
        "# with open(\"./comparison_results/deepseek_fine_tuned_times.pkl\", \"wb\") as f:\n",
        "#     pickle.dump(deepseek_fine_tuned_times, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6667128f",
      "metadata": {
        "id": "6667128f"
      },
      "outputs": [],
      "source": [
        "### Comparing Llama3 and DeepSeek Results\n",
        "\n",
        "# comparison_matrics = calc_metrics(llama3_answers_fine_tuned, deepseek_answers_fine_tuned) ### Change based on your models\n",
        "\n",
        "# with open(\"./comparison_results/comparison_matrics.pkl\", \"wb\") as f:\n",
        "#     pickle.dump(comparison_matrics, f)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "FinRL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "945551b24937492a87234945219b781f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdde208b6dc4401897f2b8a97bdb070b",
              "IPY_MODEL_dfe0079b08e54b79b29c641a7c14c02a",
              "IPY_MODEL_faf054d3370842bca00b268629876d76"
            ],
            "layout": "IPY_MODEL_6263855ed9204b9996ae3e9a6b597d36"
          }
        },
        "cdde208b6dc4401897f2b8a97bdb070b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d43e02699690474b817578d9484f8286",
            "placeholder": "",
            "style": "IPY_MODEL_3b0944fbe8b044c6b7c678524b348ce9",
            "value": "Loadingweights:100%"
          }
        },
        "dfe0079b08e54b79b29c641a7c14c02a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_194259d221ac426fb1057427c3e28dc7",
            "max": 338,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5041052c9ff4fdab4972e4b5dcffec9",
            "value": 338
          }
        },
        "faf054d3370842bca00b268629876d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c337029540a24a6e94077d971ab48bd6",
            "placeholder": "",
            "style": "IPY_MODEL_3e3da0c3a99041a7ba9522f683d858ae",
            "value": "338/338[00:44&lt;00:00,10.03it/s,Materializingparam=model.norm.weight]"
          }
        },
        "6263855ed9204b9996ae3e9a6b597d36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d43e02699690474b817578d9484f8286": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b0944fbe8b044c6b7c678524b348ce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "194259d221ac426fb1057427c3e28dc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5041052c9ff4fdab4972e4b5dcffec9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c337029540a24a6e94077d971ab48bd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e3da0c3a99041a7ba9522f683d858ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5075db52b2bf4673a02049cdb1393fcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53ef3d8207f543e88b60a81dd137ba88",
              "IPY_MODEL_935e186acfa44846b3cff1a4c44190de",
              "IPY_MODEL_79f9d365186640ea97629723d3dfb3df"
            ],
            "layout": "IPY_MODEL_5a9c85696e8e468c977f73b7f76c5333"
          }
        },
        "53ef3d8207f543e88b60a81dd137ba88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6394a5e3ca4c4f05ae9ce6a5f572cdb4",
            "placeholder": "",
            "style": "IPY_MODEL_e5b85dfef0f442cda33b46ee54b5f25f",
            "value": "Map:100%"
          }
        },
        "935e186acfa44846b3cff1a4c44190de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af7a78d622a84c71a1e45f4162de2f97",
            "max": 1230,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb7149b46ac44229aae84fdacb2c2ef7",
            "value": 1230
          }
        },
        "79f9d365186640ea97629723d3dfb3df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f658d94f7242454a89348b4a53d9cdde",
            "placeholder": "",
            "style": "IPY_MODEL_26669556bfa14b1aa0f4b841d5a85fbb",
            "value": "1230/1230[00:18&lt;00:00,67.33examples/s]"
          }
        },
        "5a9c85696e8e468c977f73b7f76c5333": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6394a5e3ca4c4f05ae9ce6a5f572cdb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5b85dfef0f442cda33b46ee54b5f25f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af7a78d622a84c71a1e45f4162de2f97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb7149b46ac44229aae84fdacb2c2ef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f658d94f7242454a89348b4a53d9cdde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26669556bfa14b1aa0f4b841d5a85fbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "176d92a9ad5041649e80558448f3efa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8415d69f8ae4d569f8d31a5146cc3a7",
              "IPY_MODEL_bdaa16b5e0524f84854c16715ce51069",
              "IPY_MODEL_607314b73a024c32b20a50eb475e65b6"
            ],
            "layout": "IPY_MODEL_1240e3acdae347ce98cadef801a9372f"
          }
        },
        "d8415d69f8ae4d569f8d31a5146cc3a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b0efea033a64ccba984155f47d12433",
            "placeholder": "",
            "style": "IPY_MODEL_c99d7f1b5e7341beab7be05ae1bb4b2d",
            "value": "Map:100%"
          }
        },
        "bdaa16b5e0524f84854c16715ce51069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_604aeadb025d41239087d900967e01c5",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_677b93894127442faf382af3bf9f2602",
            "value": 300
          }
        },
        "607314b73a024c32b20a50eb475e65b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56638d9900714f599d5ed1e0a1f816a7",
            "placeholder": "",
            "style": "IPY_MODEL_290ccb34c57f42a69c9d4c3735532430",
            "value": "300/300[00:03&lt;00:00,93.09examples/s]"
          }
        },
        "1240e3acdae347ce98cadef801a9372f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b0efea033a64ccba984155f47d12433": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c99d7f1b5e7341beab7be05ae1bb4b2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "604aeadb025d41239087d900967e01c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "677b93894127442faf382af3bf9f2602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56638d9900714f599d5ed1e0a1f816a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "290ccb34c57f42a69c9d4c3735532430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1fb2a20baf67417fb020a91c07da2baa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_563e65a5b02b43b28ea8523e66508fe9",
              "IPY_MODEL_54ce3b33427944ca964de734e723df3e",
              "IPY_MODEL_33bf9e3819314e989b4d3c4672141fc0"
            ],
            "layout": "IPY_MODEL_0d186a32574d4ed3ad6d48ee53fe9fc5"
          }
        },
        "563e65a5b02b43b28ea8523e66508fe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6ae4e71f30044108d5f302b4c56af12",
            "placeholder": "",
            "style": "IPY_MODEL_01ba9bbb4165407d8a08dcec22f33aaa",
            "value": "config.json:100%"
          }
        },
        "54ce3b33427944ca964de734e723df3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dba540cb79ff4a2694c10354d4b9e1f9",
            "max": 660,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83f3fba03d004191a888f67f6715d51e",
            "value": 660
          }
        },
        "33bf9e3819314e989b4d3c4672141fc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99ea5da0f5e844adaf7804804634f04c",
            "placeholder": "",
            "style": "IPY_MODEL_8a5c1e570b69433c896053292b7916f8",
            "value": "660/660[00:00&lt;00:00,47.6kB/s]"
          }
        },
        "0d186a32574d4ed3ad6d48ee53fe9fc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6ae4e71f30044108d5f302b4c56af12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01ba9bbb4165407d8a08dcec22f33aaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dba540cb79ff4a2694c10354d4b9e1f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83f3fba03d004191a888f67f6715d51e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99ea5da0f5e844adaf7804804634f04c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a5c1e570b69433c896053292b7916f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38ff0c3bf3a44188a2c29d6c89e5180c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e87d8c48c6c943a6801de008e08dc708",
              "IPY_MODEL_18aa7656aa7a4fef91f194d21a3eff5e",
              "IPY_MODEL_7c72deb61a0e4a7393370ac92bf800b2"
            ],
            "layout": "IPY_MODEL_0709a9c6b75b4473b7786af987c66e00"
          }
        },
        "e87d8c48c6c943a6801de008e08dc708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_619df901f8e2438eaa35088d66c72f08",
            "placeholder": "",
            "style": "IPY_MODEL_60ac0f02c8514235bdada4a2d50d72e2",
            "value": "tokenizer_config.json:"
          }
        },
        "18aa7656aa7a4fef91f194d21a3eff5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a642b48bf514e70902edd329006cbd8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6f4ecc66f254bde9c4a408dfabad45b",
            "value": 1
          }
        },
        "7c72deb61a0e4a7393370ac92bf800b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8640ed9938b4b9aab0417432d8251ec",
            "placeholder": "",
            "style": "IPY_MODEL_2cab26bd1f0c49c89d7bb3f510c63094",
            "value": "7.30k/?[00:00&lt;00:00,528kB/s]"
          }
        },
        "0709a9c6b75b4473b7786af987c66e00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "619df901f8e2438eaa35088d66c72f08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60ac0f02c8514235bdada4a2d50d72e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a642b48bf514e70902edd329006cbd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f6f4ecc66f254bde9c4a408dfabad45b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8640ed9938b4b9aab0417432d8251ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cab26bd1f0c49c89d7bb3f510c63094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "100b377bfadc41c091aa9a59909e83bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_006d24ce7f454488b991d0cac736b41b",
              "IPY_MODEL_9e9211e9d3874a6ea224237f3ff8cffa",
              "IPY_MODEL_c02174c60c1c4543a510fc98222e2738"
            ],
            "layout": "IPY_MODEL_2936c3166d1f4830a605e3ecb7f3dd0e"
          }
        },
        "006d24ce7f454488b991d0cac736b41b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa8738270d2a4b8ab2c9b609efb98271",
            "placeholder": "",
            "style": "IPY_MODEL_0f9524a9971b4ebfb482bb8be0b17b6c",
            "value": "vocab.json:"
          }
        },
        "9e9211e9d3874a6ea224237f3ff8cffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b8a9a6d1e594d9ead81ad932bfde840",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_629a9593d5094286b4fdf6b349364981",
            "value": 1
          }
        },
        "c02174c60c1c4543a510fc98222e2738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cb199d6cb5d4d7688be30557a0bd53b",
            "placeholder": "",
            "style": "IPY_MODEL_986decac36664cf79707d8924e7847be",
            "value": "2.78M/?[00:00&lt;00:00,29.1MB/s]"
          }
        },
        "2936c3166d1f4830a605e3ecb7f3dd0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa8738270d2a4b8ab2c9b609efb98271": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f9524a9971b4ebfb482bb8be0b17b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b8a9a6d1e594d9ead81ad932bfde840": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "629a9593d5094286b4fdf6b349364981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cb199d6cb5d4d7688be30557a0bd53b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "986decac36664cf79707d8924e7847be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c7dc2289e8840a08420458affc1b4de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a58abffd94f74bcc9d9e23d453b52f72",
              "IPY_MODEL_916e42fccb764de19af90139b5759ee0",
              "IPY_MODEL_380b655f0f4542678c80c8d25cefc996"
            ],
            "layout": "IPY_MODEL_436814b307b74f29ad441050dbc8b4f4"
          }
        },
        "a58abffd94f74bcc9d9e23d453b52f72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42c5579926844c8db9a33d3d5383179b",
            "placeholder": "",
            "style": "IPY_MODEL_fe6bced3789c413abaee8b242057376b",
            "value": "merges.txt:"
          }
        },
        "916e42fccb764de19af90139b5759ee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43db279c3970437191648243b00a169d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f045f9ff5ccf481c9e168552446bb628",
            "value": 1
          }
        },
        "380b655f0f4542678c80c8d25cefc996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf3fd0aa1c9f4df7adf38137b6ca830a",
            "placeholder": "",
            "style": "IPY_MODEL_304558ed5d73458091b9c3444517d589",
            "value": "1.67M/?[00:00&lt;00:00,32.1MB/s]"
          }
        },
        "436814b307b74f29ad441050dbc8b4f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42c5579926844c8db9a33d3d5383179b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe6bced3789c413abaee8b242057376b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43db279c3970437191648243b00a169d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f045f9ff5ccf481c9e168552446bb628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf3fd0aa1c9f4df7adf38137b6ca830a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "304558ed5d73458091b9c3444517d589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf70ee516f3a46f992b75ab15b7b1927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c57383da10184ffca1459f4cce846da3",
              "IPY_MODEL_c5fe9d16c36b46c28ea4076eb33c2fd3",
              "IPY_MODEL_52d6079c21114f4fb55cc70c6cb71747"
            ],
            "layout": "IPY_MODEL_5fb67329132642c5b97c7507b9c8d53c"
          }
        },
        "c57383da10184ffca1459f4cce846da3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fedc0ed5ba045dca897f0eee3d4dd78",
            "placeholder": "",
            "style": "IPY_MODEL_11c9007a17a34592b2925b18bc12091f",
            "value": "tokenizer.json:"
          }
        },
        "c5fe9d16c36b46c28ea4076eb33c2fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f0869776aa04a97b03d071ebaaa95ff",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_018b1fd05bb94129b0e2083c79e3dafe",
            "value": 1
          }
        },
        "52d6079c21114f4fb55cc70c6cb71747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75ce3e1fe2784368906df645abffe4e2",
            "placeholder": "",
            "style": "IPY_MODEL_5ebfa2e7891d44559338ad4b8d463217",
            "value": "7.03M/?[00:00&lt;00:00,64.9MB/s]"
          }
        },
        "5fb67329132642c5b97c7507b9c8d53c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fedc0ed5ba045dca897f0eee3d4dd78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11c9007a17a34592b2925b18bc12091f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f0869776aa04a97b03d071ebaaa95ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "018b1fd05bb94129b0e2083c79e3dafe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "75ce3e1fe2784368906df645abffe4e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ebfa2e7891d44559338ad4b8d463217": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b01424acf32473ea090fd0f1b94a3ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26a0a247e69b434cb9f0a60c9ea83028",
              "IPY_MODEL_4e422ba8e37c442cb12ce1b514a7dcc3",
              "IPY_MODEL_aaef335d9b6741c4a60e414f256b22b1"
            ],
            "layout": "IPY_MODEL_ec2804e72d8841b9abc89e7946d8a270"
          }
        },
        "26a0a247e69b434cb9f0a60c9ea83028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_373ce7540bb649409b66ddad50799dbd",
            "placeholder": "",
            "style": "IPY_MODEL_ef8fbac29b0c4e44b9ba5a2eb8bcdfc5",
            "value": "model.safetensors:100%"
          }
        },
        "4e422ba8e37c442cb12ce1b514a7dcc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_685dcf6742054c6199c773dae09e44a0",
            "max": 3087467144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e53b0f5f1ad4e62a32306dcf7765e13",
            "value": 3087467144
          }
        },
        "aaef335d9b6741c4a60e414f256b22b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fb2dddce2d144b9a37d3913bcf93b0d",
            "placeholder": "",
            "style": "IPY_MODEL_0fcb1cd320b8499b8d2f6cdf530e9b23",
            "value": "3.09G/3.09G[00:56&lt;00:00,184MB/s]"
          }
        },
        "ec2804e72d8841b9abc89e7946d8a270": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "373ce7540bb649409b66ddad50799dbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef8fbac29b0c4e44b9ba5a2eb8bcdfc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "685dcf6742054c6199c773dae09e44a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e53b0f5f1ad4e62a32306dcf7765e13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2fb2dddce2d144b9a37d3913bcf93b0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fcb1cd320b8499b8d2f6cdf530e9b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "382e09d1e0214b95b17457f92a6ea556": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92f60d48531d48b1b140479e6bca6bf0",
              "IPY_MODEL_d246aae51b1145fcbf3fc4c8c0c06a0a",
              "IPY_MODEL_d0a8d3170bdd467d92be30b835a8ca0e"
            ],
            "layout": "IPY_MODEL_596bff4aaf7d42d5b8b511f0ce58ca99"
          }
        },
        "92f60d48531d48b1b140479e6bca6bf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c12b5580b054cc2ad3df71691c8e907",
            "placeholder": "",
            "style": "IPY_MODEL_be6b849b38fb406380fd689d4f2f17a8",
            "value": "Loadingweights:100%"
          }
        },
        "d246aae51b1145fcbf3fc4c8c0c06a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73bcb13c452c4cf2a40def3901db2b21",
            "max": 338,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b147ca6d80254007a7ddb27e8716efe2",
            "value": 338
          }
        },
        "d0a8d3170bdd467d92be30b835a8ca0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ec600b69dcd4cce900141396f163f1a",
            "placeholder": "",
            "style": "IPY_MODEL_366f8799850c4e6d89e4e064142f0b02",
            "value": "338/338[00:06&lt;00:00,47.88it/s,Materializingparam=model.norm.weight]"
          }
        },
        "596bff4aaf7d42d5b8b511f0ce58ca99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c12b5580b054cc2ad3df71691c8e907": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be6b849b38fb406380fd689d4f2f17a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73bcb13c452c4cf2a40def3901db2b21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b147ca6d80254007a7ddb27e8716efe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5ec600b69dcd4cce900141396f163f1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "366f8799850c4e6d89e4e064142f0b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1ddddc1f82d4d2a831c4076fd5ca9b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3bec1f6d4c8c437690bd862d16374e7f",
              "IPY_MODEL_6db88547d3514e04897087990aa7c03b",
              "IPY_MODEL_240b4e03c33f42aaa1e470c15c1c7816"
            ],
            "layout": "IPY_MODEL_6fa5121bba6841cc9726ba5682124a6f"
          }
        },
        "3bec1f6d4c8c437690bd862d16374e7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8b9d29b17a044baa25cd28c3d549cce",
            "placeholder": "",
            "style": "IPY_MODEL_c0cd319c63d14468b16b47b78ac2696f",
            "value": "generation_config.json:100%"
          }
        },
        "6db88547d3514e04897087990aa7c03b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79889db8a05f4dcf9d71d211bb88b579",
            "max": 242,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ebffd6121bff49299809ff373cc95036",
            "value": 242
          }
        },
        "240b4e03c33f42aaa1e470c15c1c7816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a87b78b67f9b4c34a4b02c86fea1eedc",
            "placeholder": "",
            "style": "IPY_MODEL_8cb732a0483445d8bbfc66bb2f36b1bf",
            "value": "242/242[00:00&lt;00:00,10.6kB/s]"
          }
        },
        "6fa5121bba6841cc9726ba5682124a6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8b9d29b17a044baa25cd28c3d549cce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0cd319c63d14468b16b47b78ac2696f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79889db8a05f4dcf9d71d211bb88b579": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebffd6121bff49299809ff373cc95036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a87b78b67f9b4c34a4b02c86fea1eedc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cb732a0483445d8bbfc66bb2f36b1bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed602913e67b4753a9bb8c83ff97bc46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1caeeb98ce8a4323bbd1a75bc87a70a5",
              "IPY_MODEL_55a972c5f02c492c99358a9611627285",
              "IPY_MODEL_92f79931bbf94c14b90bf84a67eb5a91"
            ],
            "layout": "IPY_MODEL_db5d8b42fa2d4e8493b6029dce22f2fa"
          }
        },
        "1caeeb98ce8a4323bbd1a75bc87a70a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3ea88ed30e8425f8bfc2d27795d4fd7",
            "placeholder": "",
            "style": "IPY_MODEL_991d612451134bcd859482fd82ef35e8",
            "value": "Loadingweights:100%"
          }
        },
        "55a972c5f02c492c99358a9611627285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d6d6866a3a743ccb71372c1b9070e54",
            "max": 291,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b06f5a3636fd48d1af8917aef6dbf36c",
            "value": 291
          }
        },
        "92f79931bbf94c14b90bf84a67eb5a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b827c69787d4485d818f94f581e59549",
            "placeholder": "",
            "style": "IPY_MODEL_06a0c9095a2341718a69fe72a74c2cdc",
            "value": "291/291[02:07&lt;00:00,3.44it/s,Materializingparam=model.norm.weight]"
          }
        },
        "db5d8b42fa2d4e8493b6029dce22f2fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3ea88ed30e8425f8bfc2d27795d4fd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "991d612451134bcd859482fd82ef35e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d6d6866a3a743ccb71372c1b9070e54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b06f5a3636fd48d1af8917aef6dbf36c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b827c69787d4485d818f94f581e59549": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06a0c9095a2341718a69fe72a74c2cdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}